{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_cost` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss by MSE\n",
    "    # ***************************************************\n",
    "    return np.sum((y-tx@w)**2)/(2*len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "        \n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss for each combination of w0 and w1.\n",
    "    # ***************************************************\n",
    "    for i in range(len(grid_w0)):\n",
    "        print(\"evolution \"+str(i+1)+\"/\"+str(len(grid_w0))) # optional, to see it going\n",
    "        for j in range(len(grid_w1)):\n",
    "            losses[i,j] = compute_loss(y, tx, np.array([[grid_w0[i]],[grid_w1[j]]]))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evolution 1/10\n",
      "evolution 2/10\n",
      "evolution 3/10\n",
      "evolution 4/10\n",
      "evolution 5/10\n",
      "evolution 6/10\n",
      "evolution 7/10\n",
      "evolution 8/10\n",
      "evolution 9/10\n",
      "evolution 10/10\n",
      "Grid Search: loss*=2670863.5706326636, w0*=66.66666666666669, w1*=16.666666666666686, execution time=73.450 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=10)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient vector\n",
    "    # ***************************************************\n",
    "    return tx.T@(tx@w-y)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma*grad\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2792.2367127591674, w0=51.30574540147352, w1=9.435798704492393\n",
      "GD iter. 1/49: loss=265.302462108962, w0=66.69746902191565, w1=12.266538315840034\n",
      "GD iter. 2/49: loss=37.87837955044161, w0=71.31498610804833, w1=13.115760199244338\n",
      "GD iter. 3/49: loss=17.410212120174496, w0=72.70024123388814, w1=13.370526764265632\n",
      "GD iter. 4/49: loss=15.568077051450455, w0=73.11581777164008, w1=13.446956733772023\n",
      "GD iter. 5/49: loss=15.402284895265295, w0=73.24049073296567, w1=13.469885724623941\n",
      "GD iter. 6/49: loss=15.38736360120863, w0=73.27789262136332, w1=13.476764421879517\n",
      "GD iter. 7/49: loss=15.38602068474353, w0=73.28911318788263, w1=13.478828031056189\n",
      "GD iter. 8/49: loss=15.385899822261674, w0=73.29247935783842, w1=13.47944711380919\n",
      "GD iter. 9/49: loss=15.385888944638305, w0=73.29348920882516, w1=13.47963283863509\n",
      "GD iter. 10/49: loss=15.3858879656522, w0=73.29379216412119, w1=13.479688556082861\n",
      "GD iter. 11/49: loss=15.385887877543453, w0=73.29388305071, w1=13.479705271317192\n",
      "GD iter. 12/49: loss=15.385887869613667, w0=73.29391031668663, w1=13.479710285887492\n",
      "GD iter. 13/49: loss=15.385887868899983, w0=73.29391849647962, w1=13.479711790258582\n",
      "GD iter. 14/49: loss=15.38588786883575, w0=73.29392095041752, w1=13.479712241569908\n",
      "GD iter. 15/49: loss=15.385887868829974, w0=73.29392168659889, w1=13.479712376963306\n",
      "GD iter. 16/49: loss=15.38588786882945, w0=73.2939219074533, w1=13.479712417581325\n",
      "GD iter. 17/49: loss=15.385887868829403, w0=73.29392197370962, w1=13.479712429766732\n",
      "GD iter. 18/49: loss=15.3858878688294, w0=73.29392199358651, w1=13.479712433422353\n",
      "GD iter. 19/49: loss=15.385887868829398, w0=73.29392199954958, w1=13.47971243451904\n",
      "GD iter. 20/49: loss=15.385887868829398, w0=73.29392200133852, w1=13.479712434848047\n",
      "GD iter. 21/49: loss=15.3858878688294, w0=73.29392200187519, w1=13.479712434946748\n",
      "GD iter. 22/49: loss=15.3858878688294, w0=73.29392200203618, w1=13.479712434976358\n",
      "GD iter. 23/49: loss=15.3858878688294, w0=73.29392200208449, w1=13.479712434985242\n",
      "GD iter. 24/49: loss=15.3858878688294, w0=73.29392200209898, w1=13.479712434987906\n",
      "GD iter. 25/49: loss=15.385887868829398, w0=73.29392200210333, w1=13.479712434988706\n",
      "GD iter. 26/49: loss=15.3858878688294, w0=73.29392200210462, w1=13.479712434988945\n",
      "GD iter. 27/49: loss=15.3858878688294, w0=73.29392200210502, w1=13.479712434989018\n",
      "GD iter. 28/49: loss=15.3858878688294, w0=73.29392200210515, w1=13.47971243498904\n",
      "GD iter. 29/49: loss=15.3858878688294, w0=73.29392200210518, w1=13.479712434989047\n",
      "GD iter. 30/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 31/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 32/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 33/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 34/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 35/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 36/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 37/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 38/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 39/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 40/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 41/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 42/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 43/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 44/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 45/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 46/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 47/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 48/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 49/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD: execution time=0.035 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3a647c209a496aa21e93b948f90ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient computation. It's the same as the usual gradient.\n",
    "    # ***************************************************\n",
    "    return tx.T@(tx@w-y)/len(y)\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic gradient descent.\n",
    "        # ***************************************************\n",
    "        minibatch_y, minibatch_tx = batch_iter(y, tx, batch_size)\n",
    "        w = w - gamma*compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "\n",
    "        print(\"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=2197.7801527096085, w0=7.910654370443097, w1=4.002537566495382\n",
      "SGD iter. 1/49: loss=1963.7937847503179, w0=12.89441698323461, w1=-2.2910068649796997\n",
      "SGD iter. 2/49: loss=1512.0967211654868, w0=19.93080798060881, w1=1.4049565980334515\n",
      "SGD iter. 3/49: loss=1328.2990748388686, w0=24.256081564737556, w1=-1.3902767774936526\n",
      "SGD iter. 4/49: loss=1192.1747106942662, w0=27.942929552030115, w1=-3.750062068587858\n",
      "SGD iter. 5/49: loss=849.7899707339516, w0=33.96061184103122, w1=2.4479910143312127\n",
      "SGD iter. 6/49: loss=615.1316993634742, w0=39.169559323555504, w1=7.561985272294288\n",
      "SGD iter. 7/49: loss=489.4160965931314, w0=42.75715068539381, w1=9.534333492545455\n",
      "SGD iter. 8/49: loss=404.3779139772782, w0=46.08778826685505, w1=7.33070118141484\n",
      "SGD iter. 9/49: loss=287.169226745853, w0=49.97946735936301, w1=13.53340103307731\n",
      "SGD iter. 10/49: loss=215.6633224874926, w0=53.28282913991071, w1=13.812926627297092\n",
      "SGD iter. 11/49: loss=158.08067329077454, w0=56.406731433867634, w1=13.018881060277208\n",
      "SGD iter. 12/49: loss=145.99273046507875, w0=57.155433314942584, w1=12.606289234688463\n",
      "SGD iter. 13/49: loss=126.88671651514348, w0=58.390381970650964, w1=12.538356060670818\n",
      "SGD iter. 14/49: loss=116.55401596980589, w0=59.1372519012031, w1=12.092287533894284\n",
      "SGD iter. 15/49: loss=109.41386135024405, w0=59.76251871710202, w1=11.253264119524529\n",
      "SGD iter. 16/49: loss=84.02766122208463, w0=61.700435918586315, w1=11.784239893847573\n",
      "SGD iter. 17/49: loss=71.00377311468408, w0=62.755939098021706, w1=13.047639750185187\n",
      "SGD iter. 18/49: loss=67.74636999373053, w0=63.06081093916653, w1=13.546061687921084\n",
      "SGD iter. 19/49: loss=59.520418460878055, w0=65.02119260272124, w1=17.93291459396025\n",
      "SGD iter. 20/49: loss=55.64764800118661, w0=65.9648775953396, w1=18.657417365590288\n",
      "SGD iter. 21/49: loss=50.3332122046459, w0=66.84772703874393, w1=18.80335960267107\n",
      "SGD iter. 22/49: loss=57.547780673803864, w0=65.72476651489244, w1=18.678911490646172\n",
      "SGD iter. 23/49: loss=44.19129771639081, w0=66.86410647973919, w1=17.513109519763127\n",
      "SGD iter. 24/49: loss=39.90497265887619, w0=67.23871510853738, w1=16.997188668907675\n",
      "SGD iter. 25/49: loss=40.33617552616703, w0=67.05695688411187, w1=16.79646407350674\n",
      "SGD iter. 26/49: loss=24.688696378021483, w0=69.04421923199608, w1=12.741035655801328\n",
      "SGD iter. 27/49: loss=18.789754392689584, w0=70.68602607102206, w1=13.39839908823464\n",
      "SGD iter. 28/49: loss=18.904215379194486, w0=70.64287373062689, w1=13.386986586262346\n",
      "SGD iter. 29/49: loss=16.9438302559635, w0=71.56266316958636, w1=13.824136053653236\n",
      "SGD iter. 30/49: loss=17.21427282319097, w0=71.43769958501079, w1=13.939286419238991\n",
      "SGD iter. 31/49: loss=16.509372254216405, w0=72.08476404687751, w1=14.365661536283033\n",
      "SGD iter. 32/49: loss=16.221315804280756, w0=73.17342574004978, w1=12.192725052870099\n",
      "SGD iter. 33/49: loss=16.18356328321878, w0=73.19019948085587, w1=12.22090646607865\n",
      "SGD iter. 34/49: loss=15.850272526891379, w0=72.35671491465274, w1=13.255185841123511\n",
      "SGD iter. 35/49: loss=15.717518327331057, w0=72.55935580491352, w1=13.128040117416733\n",
      "SGD iter. 36/49: loss=15.962259008684821, w0=73.56164292474418, w1=12.439968337967011\n",
      "SGD iter. 37/49: loss=15.623458022563671, w0=72.94210196518759, w1=12.886953656823902\n",
      "SGD iter. 38/49: loss=17.506005956032215, w0=71.38184036284329, w1=12.715395760302984\n",
      "SGD iter. 39/49: loss=15.706995856463315, w0=72.49356702066639, w1=13.520306510344085\n",
      "SGD iter. 40/49: loss=15.600676535808505, w0=72.65894448383884, w1=13.642134370867522\n",
      "SGD iter. 41/49: loss=15.45473075724341, w0=73.27202486594302, w1=13.850126112187823\n",
      "SGD iter. 42/49: loss=15.479994711784233, w0=72.87403448504581, w1=13.588836946109957\n",
      "SGD iter. 43/49: loss=16.999626446172755, w0=74.45119836916427, w1=14.85382617297919\n",
      "SGD iter. 44/49: loss=17.166059403936945, w0=74.5811228381349, w1=14.859370754262157\n",
      "SGD iter. 45/49: loss=22.18675435720146, w0=76.19798481009714, w1=11.206355405399986\n",
      "SGD iter. 46/49: loss=17.45042694592848, w0=74.92618904604883, w1=12.269430463110744\n",
      "SGD iter. 47/49: loss=17.448499664613873, w0=74.92519693881746, w1=12.269685297928351\n",
      "SGD iter. 48/49: loss=17.640862242361802, w0=75.19908714977667, w1=12.54147232331044\n",
      "SGD iter. 49/49: loss=19.46141956999064, w0=75.92727335237474, w1=12.37675093595293\n",
      "SGD: execution time=0.053 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a1b7aad4c944e680f74463d6481069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=1, min=1), Output()), _dom_classes=('widget…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "# ***************************************************\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2869.8351145358524, w0=51.84746409844842, w1=7.7244264061924195\n",
      "GD iter. 1/49: loss=318.2821247015965, w0=67.40170332798297, w1=10.041754328050114\n",
      "GD iter. 2/49: loss=88.6423556165128, w0=72.06797509684336, w1=10.736952704607411\n",
      "GD iter. 3/49: loss=67.9747763988552, w0=73.46785662750146, w1=10.945512217574597\n",
      "GD iter. 4/49: loss=66.11469426926604, w0=73.88782108669889, w1=11.00808007146475\n",
      "GD iter. 5/49: loss=65.94728687760302, w0=74.01381042445813, w1=11.026850427631798\n",
      "GD iter. 6/49: loss=65.93222021235334, w0=74.0516072257859, w1=11.032481534481914\n",
      "GD iter. 7/49: loss=65.93086421248087, w0=74.06294626618423, w1=11.034170866536945\n",
      "GD iter. 8/49: loss=65.93074217249236, w0=74.06634797830372, w1=11.034677666153454\n",
      "GD iter. 9/49: loss=65.93073118889338, w0=74.06736849193958, w1=11.034829706038408\n",
      "GD iter. 10/49: loss=65.93073020036948, w0=74.06767464603033, w1=11.034875318003895\n",
      "GD iter. 11/49: loss=65.93073011140233, w0=74.06776649225755, w1=11.034889001593541\n",
      "GD iter. 12/49: loss=65.93073010339529, w0=74.06779404612573, w1=11.034893106670431\n",
      "GD iter. 13/49: loss=65.93073010267466, w0=74.06780231228618, w1=11.034894338193501\n",
      "GD iter. 14/49: loss=65.93073010260979, w0=74.06780479213431, w1=11.034894707650421\n",
      "GD iter. 15/49: loss=65.93073010260395, w0=74.06780553608874, w1=11.034894818487496\n",
      "GD iter. 16/49: loss=65.93073010260342, w0=74.06780575927507, w1=11.03489485173862\n",
      "GD iter. 17/49: loss=65.93073010260338, w0=74.06780582623098, w1=11.034894861713957\n",
      "GD iter. 18/49: loss=65.93073010260338, w0=74.06780584631775, w1=11.034894864706557\n",
      "GD iter. 19/49: loss=65.93073010260336, w0=74.06780585234378, w1=11.034894865604338\n",
      "GD iter. 20/49: loss=65.93073010260338, w0=74.06780585415159, w1=11.034894865873675\n",
      "GD iter. 21/49: loss=65.93073010260336, w0=74.06780585469393, w1=11.034894865954474\n",
      "GD iter. 22/49: loss=65.93073010260338, w0=74.06780585485663, w1=11.034894865978712\n",
      "GD iter. 23/49: loss=65.93073010260336, w0=74.06780585490544, w1=11.034894865985985\n",
      "GD iter. 24/49: loss=65.93073010260338, w0=74.0678058549201, w1=11.034894865988166\n",
      "GD iter. 25/49: loss=65.93073010260336, w0=74.06780585492449, w1=11.034894865988822\n",
      "GD iter. 26/49: loss=65.93073010260336, w0=74.06780585492581, w1=11.034894865989015\n",
      "GD iter. 27/49: loss=65.93073010260336, w0=74.06780585492619, w1=11.034894865989076\n",
      "GD iter. 28/49: loss=65.93073010260338, w0=74.06780585492632, w1=11.034894865989099\n",
      "GD iter. 29/49: loss=65.93073010260339, w0=74.06780585492635, w1=11.0348948659891\n",
      "GD iter. 30/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 31/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 32/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 33/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 34/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 35/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 36/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 37/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 38/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 39/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 40/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 41/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 42/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 43/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 44/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 45/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 46/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 47/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 48/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 49/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD: execution time=0.010 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points \n",
    "#       and the model fit\n",
    "# ***************************************************\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fcd4b1dc32a46beab8eed7ec34d694e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "def compute_loss_mae(y, tx, w=np.array([[1],[2]])):\n",
    "\n",
    "    \"\"\"Calculate the loss using MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss by MSE\n",
    "    return np.sum(np.abs(y-tx@w))/len(y)\n",
    "    # ***************************************************\n",
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute subgradient gradient vector for MAE\n",
    "    # ***************************************************\n",
    "    arg = tx@w-y\n",
    "    if any(np.abs(arg)<1e-20):\n",
    "        warnings.warn(\"non-differentiable point reached\")\n",
    "    return tx.T@np.sign(arg)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute subgradient and loss\n",
    "        # ***************************************************\n",
    "        loss = compute_loss_mae(y, tx, w)\n",
    "        grad = compute_subgradient_mae(y, tx, w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by subgradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma*grad\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=74.06780585492638, w0=0.7, w1=6.109524327590712e-16\n",
      "SubGD iter. 1/499: loss=73.36780585492637, w0=1.4, w1=1.2219048655181425e-15\n",
      "SubGD iter. 2/499: loss=72.66780585492637, w0=2.0999999999999996, w1=1.832857298277214e-15\n",
      "SubGD iter. 3/499: loss=71.96780585492638, w0=2.8, w1=2.443809731036285e-15\n",
      "SubGD iter. 4/499: loss=71.26780585492638, w0=3.5, w1=3.054762163795356e-15\n",
      "SubGD iter. 5/499: loss=70.56780585492638, w0=4.2, w1=3.665714596554428e-15\n",
      "SubGD iter. 6/499: loss=69.86780585492637, w0=4.9, w1=4.276667029313499e-15\n",
      "SubGD iter. 7/499: loss=69.16780585492639, w0=5.6000000000000005, w1=4.887619462072571e-15\n",
      "SubGD iter. 8/499: loss=68.46780585492637, w0=6.300000000000001, w1=5.498571894831642e-15\n",
      "SubGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000001, w1=6.109524327590714e-15\n",
      "SubGD iter. 10/499: loss=67.06780585492638, w0=7.700000000000001, w1=6.720476760349785e-15\n",
      "SubGD iter. 11/499: loss=66.36780585492637, w0=8.4, w1=7.331429193108857e-15\n",
      "SubGD iter. 12/499: loss=65.66780585492639, w0=9.1, w1=7.942381625867928e-15\n",
      "SubGD iter. 13/499: loss=64.96780585492638, w0=9.799999999999999, w1=8.553334058627e-15\n",
      "SubGD iter. 14/499: loss=64.26780585492638, w0=10.499999999999998, w1=9.164286491386072e-15\n",
      "SubGD iter. 15/499: loss=63.567805854926384, w0=11.199999999999998, w1=9.775238924145143e-15\n",
      "SubGD iter. 16/499: loss=62.867805854926374, w0=11.899999999999997, w1=1.0386191356904215e-14\n",
      "SubGD iter. 17/499: loss=62.167805854926385, w0=12.599999999999996, w1=1.0997143789663286e-14\n",
      "SubGD iter. 18/499: loss=61.46780585492638, w0=13.299999999999995, w1=1.1608096222422358e-14\n",
      "SubGD iter. 19/499: loss=60.76780585492638, w0=13.999999999999995, w1=1.2219048655181429e-14\n",
      "SubGD iter. 20/499: loss=60.067805854926384, w0=14.699999999999994, w1=1.28300010879405e-14\n",
      "SubGD iter. 21/499: loss=59.36780585492639, w0=15.399999999999993, w1=1.3440953520699572e-14\n",
      "SubGD iter. 22/499: loss=58.667805854926385, w0=16.099999999999994, w1=1.4051905953458644e-14\n",
      "SubGD iter. 23/499: loss=57.96780585492638, w0=16.799999999999994, w1=1.4662858386217714e-14\n",
      "SubGD iter. 24/499: loss=57.26780585492638, w0=17.499999999999993, w1=1.5273810818976784e-14\n",
      "SubGD iter. 25/499: loss=56.567805854926384, w0=18.199999999999992, w1=1.5884763251735854e-14\n",
      "SubGD iter. 26/499: loss=55.867805854926395, w0=18.89999999999999, w1=1.6495715684494924e-14\n",
      "SubGD iter. 27/499: loss=55.167805854926385, w0=19.59999999999999, w1=1.7106668117253994e-14\n",
      "SubGD iter. 28/499: loss=54.46780585492638, w0=20.29999999999999, w1=1.7717620550013064e-14\n",
      "SubGD iter. 29/499: loss=53.767805854926394, w0=20.99999999999999, w1=1.8328572982772134e-14\n",
      "SubGD iter. 30/499: loss=53.06780585492639, w0=21.69999999999999, w1=1.8939525415531204e-14\n",
      "SubGD iter. 31/499: loss=52.367805854926395, w0=22.399999999999988, w1=1.9550477848290273e-14\n",
      "SubGD iter. 32/499: loss=51.667805854926385, w0=23.099999999999987, w1=2.0161430281049343e-14\n",
      "SubGD iter. 33/499: loss=50.96780585492639, w0=23.799999999999986, w1=2.0772382713808413e-14\n",
      "SubGD iter. 34/499: loss=50.267805854926394, w0=24.499999999999986, w1=2.1383335146567483e-14\n",
      "SubGD iter. 35/499: loss=49.56780585492639, w0=25.199999999999985, w1=2.1994287579326553e-14\n",
      "SubGD iter. 36/499: loss=48.867805854926395, w0=25.899999999999984, w1=2.2605240012085623e-14\n",
      "SubGD iter. 37/499: loss=48.1678058549264, w0=26.599999999999984, w1=2.3216192444844693e-14\n",
      "SubGD iter. 38/499: loss=47.4678058549264, w0=27.299999999999983, w1=2.3827144877603763e-14\n",
      "SubGD iter. 39/499: loss=46.7678058549264, w0=27.999999999999982, w1=2.4438097310362833e-14\n",
      "SubGD iter. 40/499: loss=46.067805854926405, w0=28.69999999999998, w1=2.5049049743121903e-14\n",
      "SubGD iter. 41/499: loss=45.367805854926395, w0=29.39999999999998, w1=2.5660002175880973e-14\n",
      "SubGD iter. 42/499: loss=44.6678058549264, w0=30.09999999999998, w1=2.6270954608640043e-14\n",
      "SubGD iter. 43/499: loss=43.9678058549264, w0=30.79999999999998, w1=2.6881907041399113e-14\n",
      "SubGD iter. 44/499: loss=43.2678058549264, w0=31.49999999999998, w1=2.7492859474158183e-14\n",
      "SubGD iter. 45/499: loss=42.567805854926405, w0=32.19999999999998, w1=2.8103811906917253e-14\n",
      "SubGD iter. 46/499: loss=41.867805854926395, w0=32.899999999999984, w1=2.871476433967632e-14\n",
      "SubGD iter. 47/499: loss=41.167805854926385, w0=33.59999999999999, w1=2.9325716772435396e-14\n",
      "SubGD iter. 48/499: loss=40.46780585492639, w0=34.29999999999999, w1=2.993666920519447e-14\n",
      "SubGD iter. 49/499: loss=39.767805854926394, w0=34.99999999999999, w1=3.054762163795354e-14\n",
      "SubGD iter. 50/499: loss=39.067805854926384, w0=35.699999999999996, w1=3.1158574070712615e-14\n",
      "SubGD iter. 51/499: loss=38.36780585492638, w0=36.4, w1=3.176952650347169e-14\n",
      "SubGD iter. 52/499: loss=37.66780585492638, w0=37.1, w1=3.238047893623076e-14\n",
      "SubGD iter. 53/499: loss=36.96780585492638, w0=37.800000000000004, w1=3.2991431368989835e-14\n",
      "SubGD iter. 54/499: loss=36.26780585492637, w0=38.50000000000001, w1=3.360238380174891e-14\n",
      "SubGD iter. 55/499: loss=35.56780585492637, w0=39.20000000000001, w1=3.421333623450798e-14\n",
      "SubGD iter. 56/499: loss=34.86780585492637, w0=39.90000000000001, w1=3.4824288667267054e-14\n",
      "SubGD iter. 57/499: loss=34.16780585492637, w0=40.600000000000016, w1=3.543524110002613e-14\n",
      "SubGD iter. 58/499: loss=33.46780585492636, w0=41.30000000000002, w1=3.60461935327852e-14\n",
      "SubGD iter. 59/499: loss=32.767805854926365, w0=42.00000000000002, w1=3.6657145965544273e-14\n",
      "SubGD iter. 60/499: loss=32.067805854926355, w0=42.700000000000024, w1=3.7268098398303347e-14\n",
      "SubGD iter. 61/499: loss=31.36780585492636, w0=43.40000000000003, w1=3.787905083106242e-14\n",
      "SubGD iter. 62/499: loss=30.66780585492635, w0=44.10000000000003, w1=3.849000326382149e-14\n",
      "SubGD iter. 63/499: loss=29.967805854926347, w0=44.80000000000003, w1=3.9100955696580566e-14\n",
      "SubGD iter. 64/499: loss=29.267805854926348, w0=45.500000000000036, w1=3.971190812933964e-14\n",
      "SubGD iter. 65/499: loss=28.567805854926345, w0=46.20000000000004, w1=4.032286056209871e-14\n",
      "SubGD iter. 66/499: loss=27.867805854926342, w0=46.90000000000004, w1=4.0933812994857785e-14\n",
      "SubGD iter. 67/499: loss=27.17327020966892, w0=47.59306930693074, w1=0.011147845678271063\n",
      "SubGD iter. 68/499: loss=26.490451563751197, w0=48.279207920792125, w1=0.03308574108989941\n",
      "SubGD iter. 69/499: loss=25.81721232277017, w0=48.96534653465351, w1=0.05502363650152776\n",
      "SubGD iter. 70/499: loss=25.15503943465645, w0=49.63069306930698, w1=0.10538326388307814\n",
      "SubGD iter. 71/499: loss=24.524103413894778, w0=50.28910891089114, w1=0.16746568532793435\n",
      "SubGD iter. 72/499: loss=23.899295346035593, w0=50.947524752475296, w1=0.22954810677279056\n",
      "SubGD iter. 73/499: loss=23.284392925657144, w0=51.59207920792084, w1=0.31242512932747524\n",
      "SubGD iter. 74/499: loss=22.686876444181845, w0=52.22277227722777, w1=0.4119501328839991\n",
      "SubGD iter. 75/499: loss=22.10626756964055, w0=52.84653465346539, w1=0.5208167847923756\n",
      "SubGD iter. 76/499: loss=21.537818828008433, w0=53.4564356435644, w1=0.6457900912635992\n",
      "SubGD iter. 77/499: loss=20.986339874628463, w0=54.0594059405941, w1=0.7796904498577214\n",
      "SubGD iter. 78/499: loss=20.445560936620446, w0=54.655445544554496, w1=0.9197570104995693\n",
      "SubGD iter. 79/499: loss=19.91191015895785, w0=55.24455445544559, w1=1.0670920297849913\n",
      "SubGD iter. 80/499: loss=19.389644090563234, w0=55.819801980198065, w1=1.2261255948210765\n",
      "SubGD iter. 81/499: loss=18.887989064395885, w0=56.36732673267331, w1=1.410709342622213\n",
      "SubGD iter. 82/499: loss=18.415960501854236, w0=56.900990099009945, w1=1.605853732220269\n",
      "SubGD iter. 83/499: loss=17.954898543040386, w0=57.42772277227727, w1=1.808762802293962\n",
      "SubGD iter. 84/499: loss=17.505757656579824, w0=57.933663366336674, w1=2.0285064197514697\n",
      "SubGD iter. 85/499: loss=17.07495742693161, w0=58.43267326732677, w1=2.2494370848672776\n",
      "SubGD iter. 86/499: loss=16.652967297509903, w0=58.91089108910895, w1=2.4837982986028337\n",
      "SubGD iter. 87/499: loss=16.24854073149673, w0=59.382178217821824, w1=2.7260245553531504\n",
      "SubGD iter. 88/499: loss=15.849105212654159, w0=59.83960396039608, w1=2.978742333469136\n",
      "SubGD iter. 89/499: loss=15.46691979123133, w0=60.262376237623805, w1=3.251528669355438\n",
      "SubGD iter. 90/499: loss=15.108294621512215, w0=60.67821782178222, w1=3.5270865794242794\n",
      "SubGD iter. 91/499: loss=14.754896345922832, w0=61.087128712871326, w1=3.806459183951815\n",
      "SubGD iter. 92/499: loss=14.40452896162028, w0=61.49603960396043, w1=4.085831788479351\n",
      "SubGD iter. 93/499: loss=14.055787028127279, w0=61.891089108910926, w1=4.373839384328607\n",
      "SubGD iter. 94/499: loss=13.714620911605635, w0=62.27920792079211, w1=4.666037469532047\n",
      "SubGD iter. 95/499: loss=13.381236307284155, w0=62.65346534653469, w1=4.959829093241769\n",
      "SubGD iter. 96/499: loss=13.058821615166238, w0=63.02079207920796, w1=5.25705719205664\n",
      "SubGD iter. 97/499: loss=12.74025172433924, w0=63.38118811881192, w1=5.560434316352406\n",
      "SubGD iter. 98/499: loss=12.42321888875611, w0=63.74158415841588, w1=5.863811440648173\n",
      "SubGD iter. 99/499: loss=12.107561731901173, w0=64.08811881188123, w1=6.172402175278548\n",
      "SubGD iter. 100/499: loss=11.800622097398135, w0=64.42772277227726, w1=6.486369310516498\n",
      "SubGD iter. 101/499: loss=11.495041794646427, w0=64.7673267326733, w1=6.800336445754448\n",
      "SubGD iter. 102/499: loss=11.189461491894715, w0=65.10693069306933, w1=7.114303580992399\n",
      "SubGD iter. 103/499: loss=10.883881189143004, w0=65.44653465346536, w1=7.428270716230349\n",
      "SubGD iter. 104/499: loss=10.584593408313202, w0=65.76534653465349, w1=7.747893210218626\n",
      "SubGD iter. 105/499: loss=10.295816534318941, w0=66.070297029703, w1=8.073669686866905\n",
      "SubGD iter. 106/499: loss=10.01135208122136, w0=66.37524752475251, w1=8.399446163515185\n",
      "SubGD iter. 107/499: loss=9.72808432666813, w0=66.6663366336634, w1=8.73297028041739\n",
      "SubGD iter. 108/499: loss=9.44812546112251, w0=66.9574257425743, w1=9.066494397319596\n",
      "SubGD iter. 109/499: loss=9.17104110409667, w0=67.23465346534658, w1=9.39863031947029\n",
      "SubGD iter. 110/499: loss=8.903656131158964, w0=67.51188118811886, w1=9.730766241620982\n",
      "SubGD iter. 111/499: loss=8.636271158221257, w0=67.78910891089114, w1=10.062902163771675\n",
      "SubGD iter. 112/499: loss=8.376151920302375, w0=68.06633663366343, w1=10.363999289979422\n",
      "SubGD iter. 113/499: loss=8.140540838751496, w0=68.32970297029709, w1=10.660466909273612\n",
      "SubGD iter. 114/499: loss=7.918544501597273, w0=68.59306930693076, w1=10.943174379960814\n",
      "SubGD iter. 115/499: loss=7.705279728377, w0=68.85643564356442, w1=11.225881850648015\n",
      "SubGD iter. 116/499: loss=7.493695831178641, w0=69.11287128712878, w1=11.504395843582206\n",
      "SubGD iter. 117/499: loss=7.289992405743416, w0=69.35544554455453, w1=11.78820189306775\n",
      "SubGD iter. 118/499: loss=7.097234035781543, w0=69.58415841584166, w1=12.060911465190971\n",
      "SubGD iter. 119/499: loss=6.919905294668923, w0=69.80594059405948, w1=12.324245668386048\n",
      "SubGD iter. 120/499: loss=6.750573527315454, w0=70.0277227722773, w1=12.587579871581125\n",
      "SubGD iter. 121/499: loss=6.584744810805664, w0=70.25643564356443, w1=12.824765405096484\n",
      "SubGD iter. 122/499: loss=6.430343276347806, w0=70.47821782178225, w1=13.065616959310148\n",
      "SubGD iter. 123/499: loss=6.278071481890353, w0=70.69306930693077, w1=13.302953389983912\n",
      "SubGD iter. 124/499: loss=6.133663329263324, w0=70.89405940594067, w1=13.525403099312918\n",
      "SubGD iter. 125/499: loss=6.00584079834303, w0=71.08811881188126, w1=13.742945617944212\n",
      "SubGD iter. 126/499: loss=5.885021825223219, w0=71.27524752475254, w1=13.953548196006844\n",
      "SubGD iter. 127/499: loss=5.771635252269658, w0=71.46237623762383, w1=14.164150774069476\n",
      "SubGD iter. 128/499: loss=5.667162061790257, w0=71.62178217821788, w1=14.349779559473173\n",
      "SubGD iter. 129/499: loss=5.586726765993146, w0=71.75346534653471, w1=14.51689010761231\n",
      "SubGD iter. 130/499: loss=5.523847812160388, w0=71.87128712871292, w1=14.670791185324186\n",
      "SubGD iter. 131/499: loss=5.480093708591872, w0=71.95445544554461, w1=14.780276456654521\n",
      "SubGD iter. 132/499: loss=5.4530880035020255, w0=72.0376237623763, w1=14.889761727984856\n",
      "SubGD iter. 133/499: loss=5.427392630862905, w0=72.10693069306937, w1=14.985916181776727\n",
      "SubGD iter. 134/499: loss=5.407322445682752, w0=72.17623762376245, w1=15.082070635568597\n",
      "SubGD iter. 135/499: loss=5.387252260502599, w0=72.24554455445552, w1=15.178225089360467\n",
      "SubGD iter. 136/499: loss=5.3704607803386955, w0=72.30099009900998, w1=15.25972348971591\n",
      "SubGD iter. 137/499: loss=5.357406523334741, w0=72.34950495049513, w1=15.335091856448138\n",
      "SubGD iter. 138/499: loss=5.345929264022584, w0=72.39801980198028, w1=15.410460223180365\n",
      "SubGD iter. 139/499: loss=5.335714659517473, w0=72.43267326732682, w1=15.469961786755725\n",
      "SubGD iter. 140/499: loss=5.330043910465361, w0=72.46039603960405, w1=15.51864528583281\n",
      "SubGD iter. 141/499: loss=5.325676428273225, w0=72.48811881188128, w1=15.561592159086487\n",
      "SubGD iter. 142/499: loss=5.322176726526591, w0=72.5019801980199, w1=15.597828332032526\n",
      "SubGD iter. 143/499: loss=5.320111309643114, w0=72.52277227722782, w1=15.624722856626713\n",
      "SubGD iter. 144/499: loss=5.318478284898438, w0=72.55049504950505, w1=15.642690329098\n",
      "SubGD iter. 145/499: loss=5.3172400485651465, w0=72.56435643564366, w1=15.664356578291091\n",
      "SubGD iter. 146/499: loss=5.316406547951545, w0=72.58514851485158, w1=15.677095775361284\n",
      "SubGD iter. 147/499: loss=5.315557122666144, w0=72.6059405940595, w1=15.689834972431477\n",
      "SubGD iter. 148/499: loss=5.31470769738074, w0=72.62673267326743, w1=15.70257416950167\n",
      "SubGD iter. 149/499: loss=5.313876880922167, w0=72.64059405940604, w1=15.72424041869476\n",
      "SubGD iter. 150/499: loss=5.3130522468713846, w0=72.66138613861396, w1=15.736979615764954\n",
      "SubGD iter. 151/499: loss=5.312377839024387, w0=72.66831683168327, w1=15.74811029423128\n",
      "SubGD iter. 152/499: loss=5.312132229725043, w0=72.67524752475258, w1=15.759240972697606\n",
      "SubGD iter. 153/499: loss=5.311886620425697, w0=72.68217821782189, w1=15.770371651163932\n",
      "SubGD iter. 154/499: loss=5.311683566098433, w0=72.68217821782189, w1=15.774323911906686\n",
      "SubGD iter. 155/499: loss=5.311661251291322, w0=72.68217821782189, w1=15.77827617264944\n",
      "SubGD iter. 156/499: loss=5.311638936484209, w0=72.68217821782189, w1=15.782228433392193\n",
      "SubGD iter. 157/499: loss=5.311616621677096, w0=72.68217821782189, w1=15.786180694134947\n",
      "SubGD iter. 158/499: loss=5.311594306869985, w0=72.68217821782189, w1=15.7901329548777\n",
      "SubGD iter. 159/499: loss=5.311571992062872, w0=72.68217821782189, w1=15.794085215620454\n",
      "SubGD iter. 160/499: loss=5.311549677255758, w0=72.68217821782189, w1=15.798037476363207\n",
      "SubGD iter. 161/499: loss=5.311527362448647, w0=72.68217821782189, w1=15.801989737105961\n",
      "SubGD iter. 162/499: loss=5.311505047641534, w0=72.68217821782189, w1=15.805941997848715\n",
      "SubGD iter. 163/499: loss=5.311482732834422, w0=72.68217821782189, w1=15.809894258591468\n",
      "SubGD iter. 164/499: loss=5.311460418027309, w0=72.68217821782189, w1=15.813846519334222\n",
      "SubGD iter. 165/499: loss=5.311438103220198, w0=72.68217821782189, w1=15.817798780076975\n",
      "SubGD iter. 166/499: loss=5.311415788413085, w0=72.68217821782189, w1=15.821751040819729\n",
      "SubGD iter. 167/499: loss=5.311393473605972, w0=72.68217821782189, w1=15.825703301562482\n",
      "SubGD iter. 168/499: loss=5.31137115879886, w0=72.68217821782189, w1=15.829655562305236\n",
      "SubGD iter. 169/499: loss=5.311348843991747, w0=72.68217821782189, w1=15.83360782304799\n",
      "SubGD iter. 170/499: loss=5.311326529184636, w0=72.68217821782189, w1=15.837560083790743\n",
      "SubGD iter. 171/499: loss=5.311304214377523, w0=72.68217821782189, w1=15.841512344533497\n",
      "SubGD iter. 172/499: loss=5.311281899570409, w0=72.68217821782189, w1=15.84546460527625\n",
      "SubGD iter. 173/499: loss=5.311259584763298, w0=72.68217821782189, w1=15.849416866019004\n",
      "SubGD iter. 174/499: loss=5.311237269956185, w0=72.68217821782189, w1=15.853369126761757\n",
      "SubGD iter. 175/499: loss=5.311214955149072, w0=72.68217821782189, w1=15.857321387504511\n",
      "SubGD iter. 176/499: loss=5.31119264034196, w0=72.68217821782189, w1=15.861273648247264\n",
      "SubGD iter. 177/499: loss=5.311170325534848, w0=72.68217821782189, w1=15.865225908990018\n",
      "SubGD iter. 178/499: loss=5.311148010727736, w0=72.68217821782189, w1=15.869178169732772\n",
      "SubGD iter. 179/499: loss=5.311125695920624, w0=72.68217821782189, w1=15.873130430475525\n",
      "SubGD iter. 180/499: loss=5.31110338111351, w0=72.68217821782189, w1=15.877082691218279\n",
      "SubGD iter. 181/499: loss=5.311081066306399, w0=72.68217821782189, w1=15.881034951961032\n",
      "SubGD iter. 182/499: loss=5.311058751499285, w0=72.68217821782189, w1=15.884987212703786\n",
      "SubGD iter. 183/499: loss=5.3110364366921745, w0=72.68217821782189, w1=15.88893947344654\n",
      "SubGD iter. 184/499: loss=5.311014121885061, w0=72.68217821782189, w1=15.892891734189293\n",
      "SubGD iter. 185/499: loss=5.310991807077948, w0=72.68217821782189, w1=15.896843994932047\n",
      "SubGD iter. 186/499: loss=5.310969492270837, w0=72.68217821782189, w1=15.9007962556748\n",
      "SubGD iter. 187/499: loss=5.310947177463723, w0=72.68217821782189, w1=15.904748516417554\n",
      "SubGD iter. 188/499: loss=5.310924862656612, w0=72.68217821782189, w1=15.908700777160307\n",
      "SubGD iter. 189/499: loss=5.310902547849499, w0=72.68217821782189, w1=15.91265303790306\n",
      "SubGD iter. 190/499: loss=5.310913706061381, w0=72.67524752475258, w1=15.910526938117323\n",
      "SubGD iter. 191/499: loss=5.310892237186269, w0=72.67524752475258, w1=15.914479198860077\n",
      "SubGD iter. 192/499: loss=5.3108699223791564, w0=72.67524752475258, w1=15.91843145960283\n",
      "SubGD iter. 193/499: loss=5.310862636053835, w0=72.66831683168327, w1=15.916305359817093\n",
      "SubGD iter. 194/499: loss=5.310859611715927, w0=72.66831683168327, w1=15.920257620559847\n",
      "SubGD iter. 195/499: loss=5.310837296908816, w0=72.66831683168327, w1=15.9242098813026\n",
      "SubGD iter. 196/499: loss=5.310814982101703, w0=72.66831683168327, w1=15.928162142045354\n",
      "SubGD iter. 197/499: loss=5.310823570190169, w0=72.66138613861396, w1=15.926036042259616\n",
      "SubGD iter. 198/499: loss=5.310804671438473, w0=72.66138613861396, w1=15.92998830300237\n",
      "SubGD iter. 199/499: loss=5.3107823566313614, w0=72.66138613861396, w1=15.933940563745123\n",
      "SubGD iter. 200/499: loss=5.310772500182623, w0=72.65445544554466, w1=15.931814463959386\n",
      "SubGD iter. 201/499: loss=5.3107720459681325, w0=72.65445544554466, w1=15.93576672470214\n",
      "SubGD iter. 202/499: loss=5.310749731161021, w0=72.65445544554466, w1=15.939718985444893\n",
      "SubGD iter. 203/499: loss=5.310727416353908, w0=72.65445544554466, w1=15.943671246187646\n",
      "SubGD iter. 204/499: loss=5.310733434318958, w0=72.64752475247535, w1=15.941545146401909\n",
      "SubGD iter. 205/499: loss=5.310717105690679, w0=72.64752475247535, w1=15.945497407144662\n",
      "SubGD iter. 206/499: loss=5.3106947908835656, w0=72.64752475247535, w1=15.949449667887416\n",
      "SubGD iter. 207/499: loss=5.310682364311412, w0=72.64059405940604, w1=15.947323568101679\n",
      "SubGD iter. 208/499: loss=5.310684480220337, w0=72.64059405940604, w1=15.951275828844432\n",
      "SubGD iter. 209/499: loss=5.310662165413225, w0=72.64059405940604, w1=15.955228089587186\n",
      "SubGD iter. 210/499: loss=5.310639850606112, w0=72.64059405940604, w1=15.95918035032994\n",
      "SubGD iter. 211/499: loss=5.310643298447746, w0=72.63366336633673, w1=15.957054250544202\n",
      "SubGD iter. 212/499: loss=5.310629539942882, w0=72.63366336633673, w1=15.961006511286955\n",
      "SubGD iter. 213/499: loss=5.3106072251357705, w0=72.63366336633673, w1=15.964958772029709\n",
      "SubGD iter. 214/499: loss=5.3105922284402, w0=72.62673267326743, w1=15.962832672243971\n",
      "SubGD iter. 215/499: loss=5.310633183099026, w0=72.63366336633673, w1=15.967301372051375\n",
      "SubGD iter. 216/499: loss=5.3105993435850625, w0=72.62673267326743, w1=15.965175272265638\n",
      "SubGD iter. 217/499: loss=5.31061822827579, w0=72.63366336633673, w1=15.969643972073042\n",
      "SubGD iter. 218/499: loss=5.310606458729927, w0=72.62673267326743, w1=15.967517872287305\n",
      "SubGD iter. 219/499: loss=5.3106032734525535, w0=72.63366336633673, w1=15.971986572094709\n",
      "SubGD iter. 220/499: loss=5.310613573874789, w0=72.62673267326743, w1=15.969860472308971\n",
      "SubGD iter. 221/499: loss=5.310588318629318, w0=72.63366336633673, w1=15.974329172116375\n",
      "SubGD iter. 222/499: loss=5.310620689019651, w0=72.62673267326743, w1=15.972203072330638\n",
      "SubGD iter. 223/499: loss=5.3105749661498844, w0=72.62673267326743, w1=15.970593411609551\n",
      "SubGD iter. 224/499: loss=5.31058363964973, w0=72.63366336633673, w1=15.975062111416955\n",
      "SubGD iter. 225/499: loss=5.310622915165495, w0=72.62673267326743, w1=15.972936011631218\n",
      "SubGD iter. 226/499: loss=5.310576651555033, w0=72.62673267326743, w1=15.97132635091013\n",
      "SubGD iter. 227/499: loss=5.310578960670142, w0=72.63366336633673, w1=15.975795050717535\n",
      "SubGD iter. 228/499: loss=5.310625141311338, w0=72.62673267326743, w1=15.973668950931797\n",
      "SubGD iter. 229/499: loss=5.31057833696018, w0=72.62673267326743, w1=15.97205929021071\n",
      "SubGD iter. 230/499: loss=5.310574635520699, w0=72.62673267326743, w1=15.970449629489623\n",
      "SubGD iter. 231/499: loss=5.310584557534202, w0=72.63366336633673, w1=15.974918329297028\n",
      "SubGD iter. 232/499: loss=5.31062247845816, w0=72.62673267326743, w1=15.97279222951129\n",
      "SubGD iter. 233/499: loss=5.310576320925847, w0=72.62673267326743, w1=15.971182568790203\n",
      "SubGD iter. 234/499: loss=5.3105798785546146, w0=72.63366336633673, w1=15.975651268597607\n",
      "SubGD iter. 235/499: loss=5.310624704604003, w0=72.62673267326743, w1=15.97352516881187\n",
      "SubGD iter. 236/499: loss=5.310578006330994, w0=72.62673267326743, w1=15.971915508090783\n",
      "SubGD iter. 237/499: loss=5.310575199575027, w0=72.63366336633673, w1=15.976384207898187\n",
      "SubGD iter. 238/499: loss=5.310626930749847, w0=72.62673267326743, w1=15.97425810811245\n",
      "SubGD iter. 239/499: loss=5.31057969173614, w0=72.62673267326743, w1=15.972648447391363\n",
      "SubGD iter. 240/499: loss=5.310575990296659, w0=72.62673267326743, w1=15.971038786670276\n",
      "SubGD iter. 241/499: loss=5.310580796439088, w0=72.63366336633673, w1=15.97550748647768\n",
      "SubGD iter. 242/499: loss=5.310624267896667, w0=72.62673267326743, w1=15.973381386691942\n",
      "SubGD iter. 243/499: loss=5.310577675701808, w0=72.62673267326743, w1=15.971771725970855\n",
      "SubGD iter. 244/499: loss=5.310576117459501, w0=72.63366336633673, w1=15.97624042577826\n",
      "SubGD iter. 245/499: loss=5.31062649404251, w0=72.62673267326743, w1=15.974114325992522\n",
      "SubGD iter. 246/499: loss=5.310579361106954, w0=72.62673267326743, w1=15.972504665271435\n",
      "SubGD iter. 247/499: loss=5.310575659667472, w0=72.62673267326743, w1=15.970895004550348\n",
      "SubGD iter. 248/499: loss=5.310581714323563, w0=72.63366336633673, w1=15.975363704357752\n",
      "SubGD iter. 249/499: loss=5.310623831189332, w0=72.62673267326743, w1=15.973237604572015\n",
      "SubGD iter. 250/499: loss=5.310577345072619, w0=72.62673267326743, w1=15.971627943850928\n",
      "SubGD iter. 251/499: loss=5.310577035343975, w0=72.63366336633673, w1=15.976096643658332\n",
      "SubGD iter. 252/499: loss=5.310626057335176, w0=72.62673267326743, w1=15.973970543872595\n",
      "SubGD iter. 253/499: loss=5.310579030477766, w0=72.62673267326743, w1=15.972360883151508\n",
      "SubGD iter. 254/499: loss=5.3105753290382856, w0=72.62673267326743, w1=15.97075122243042\n",
      "SubGD iter. 255/499: loss=5.310582632208036, w0=72.63366336633673, w1=15.975219922237825\n",
      "SubGD iter. 256/499: loss=5.310623394481999, w0=72.62673267326743, w1=15.973093822452087\n",
      "SubGD iter. 257/499: loss=5.3105770144434326, w0=72.62673267326743, w1=15.971484161731\n",
      "SubGD iter. 258/499: loss=5.3105779532284485, w0=72.63366336633673, w1=15.975952861538405\n",
      "SubGD iter. 259/499: loss=5.3106256206278415, w0=72.62673267326743, w1=15.973826761752667\n",
      "SubGD iter. 260/499: loss=5.31057869984858, w0=72.62673267326743, w1=15.97221710103158\n",
      "SubGD iter. 261/499: loss=5.310574998409098, w0=72.62673267326743, w1=15.970607440310493\n",
      "SubGD iter. 262/499: loss=5.3105835500925105, w0=72.63366336633673, w1=15.975076140117897\n",
      "SubGD iter. 263/499: loss=5.3106229577746635, w0=72.62673267326743, w1=15.97295004033216\n",
      "SubGD iter. 264/499: loss=5.310576683814244, w0=72.62673267326743, w1=15.971340379611073\n",
      "SubGD iter. 265/499: loss=5.310578871112923, w0=72.63366336633673, w1=15.975809079418477\n",
      "SubGD iter. 266/499: loss=5.310625183920505, w0=72.62673267326743, w1=15.97368297963274\n",
      "SubGD iter. 267/499: loss=5.310578369219393, w0=72.62673267326743, w1=15.972073318911653\n",
      "SubGD iter. 268/499: loss=5.310574667779911, w0=72.62673267326743, w1=15.970463658190566\n",
      "SubGD iter. 269/499: loss=5.310584467976984, w0=72.63366336633673, w1=15.97493235799797\n",
      "SubGD iter. 270/499: loss=5.3106225210673275, w0=72.62673267326743, w1=15.972806258212232\n",
      "SubGD iter. 271/499: loss=5.310576353185058, w0=72.62673267326743, w1=15.971196597491145\n",
      "SubGD iter. 272/499: loss=5.310579788997396, w0=72.63366336633673, w1=15.97566529729855\n",
      "SubGD iter. 273/499: loss=5.310624747213171, w0=72.62673267326743, w1=15.973539197512812\n",
      "SubGD iter. 274/499: loss=5.310578038590206, w0=72.62673267326743, w1=15.971929536791725\n",
      "SubGD iter. 275/499: loss=5.310575110017808, w0=72.63366336633673, w1=15.97639823659913\n",
      "SubGD iter. 276/499: loss=5.310626973359014, w0=72.62673267326743, w1=15.974272136813392\n",
      "SubGD iter. 277/499: loss=5.310579723995353, w0=72.62673267326743, w1=15.972662476092305\n",
      "SubGD iter. 278/499: loss=5.310576022555871, w0=72.62673267326743, w1=15.971052815371218\n",
      "SubGD iter. 279/499: loss=5.31058070688187, w0=72.63366336633673, w1=15.975521515178622\n",
      "SubGD iter. 280/499: loss=5.310624310505837, w0=72.62673267326743, w1=15.973395415392885\n",
      "SubGD iter. 281/499: loss=5.310577707961019, w0=72.62673267326743, w1=15.971785754671798\n",
      "SubGD iter. 282/499: loss=5.310576027902282, w0=72.63366336633673, w1=15.976254454479202\n",
      "SubGD iter. 283/499: loss=5.31062653665168, w0=72.62673267326743, w1=15.974128354693464\n",
      "SubGD iter. 284/499: loss=5.310579393366166, w0=72.62673267326743, w1=15.972518693972377\n",
      "SubGD iter. 285/499: loss=5.310575691926685, w0=72.62673267326743, w1=15.97090903325129\n",
      "SubGD iter. 286/499: loss=5.3105816247663435, w0=72.63366336633673, w1=15.975377733058695\n",
      "SubGD iter. 287/499: loss=5.310623873798502, w0=72.62673267326743, w1=15.973251633272957\n",
      "SubGD iter. 288/499: loss=5.310577377331832, w0=72.62673267326743, w1=15.97164197255187\n",
      "SubGD iter. 289/499: loss=5.310576945786757, w0=72.63366336633673, w1=15.976110672359274\n",
      "SubGD iter. 290/499: loss=5.3106260999443435, w0=72.62673267326743, w1=15.973984572573537\n",
      "SubGD iter. 291/499: loss=5.310579062736979, w0=72.62673267326743, w1=15.97237491185245\n",
      "SubGD iter. 292/499: loss=5.310575361297499, w0=72.62673267326743, w1=15.970765251131363\n",
      "SubGD iter. 293/499: loss=5.310582542650818, w0=72.63366336633673, w1=15.975233950938767\n",
      "SubGD iter. 294/499: loss=5.310623437091167, w0=72.62673267326743, w1=15.97310785115303\n",
      "SubGD iter. 295/499: loss=5.310577046702645, w0=72.62673267326743, w1=15.971498190431943\n",
      "SubGD iter. 296/499: loss=5.31057786367123, w0=72.63366336633673, w1=15.975966890239347\n",
      "SubGD iter. 297/499: loss=5.310625663237008, w0=72.62673267326743, w1=15.97384079045361\n",
      "SubGD iter. 298/499: loss=5.310578732107793, w0=72.62673267326743, w1=15.972231129732522\n",
      "SubGD iter. 299/499: loss=5.310575030668312, w0=72.62673267326743, w1=15.970621469011435\n",
      "SubGD iter. 300/499: loss=5.31058346053529, w0=72.63366336633673, w1=15.97509016881884\n",
      "SubGD iter. 301/499: loss=5.310623000383831, w0=72.62673267326743, w1=15.972964069033102\n",
      "SubGD iter. 302/499: loss=5.3105767160734585, w0=72.62673267326743, w1=15.971354408312015\n",
      "SubGD iter. 303/499: loss=5.310578781555702, w0=72.63366336633673, w1=15.97582310811942\n",
      "SubGD iter. 304/499: loss=5.310625226529674, w0=72.62673267326743, w1=15.973697008333682\n",
      "SubGD iter. 305/499: loss=5.3105784014786055, w0=72.62673267326743, w1=15.972087347612595\n",
      "SubGD iter. 306/499: loss=5.310574700039124, w0=72.62673267326743, w1=15.970477686891508\n",
      "SubGD iter. 307/499: loss=5.310584378419764, w0=72.63366336633673, w1=15.974946386698912\n",
      "SubGD iter. 308/499: loss=5.310622563676499, w0=72.62673267326743, w1=15.972820286913175\n",
      "SubGD iter. 309/499: loss=5.3105763854442705, w0=72.62673267326743, w1=15.971210626192088\n",
      "SubGD iter. 310/499: loss=5.310579699440178, w0=72.63366336633673, w1=15.975679325999492\n",
      "SubGD iter. 311/499: loss=5.31062478982234, w0=72.62673267326743, w1=15.973553226213754\n",
      "SubGD iter. 312/499: loss=5.3105780708494175, w0=72.62673267326743, w1=15.971943565492667\n",
      "SubGD iter. 313/499: loss=5.310575020460591, w0=72.63366336633673, w1=15.976412265300072\n",
      "SubGD iter. 314/499: loss=5.3106270159681825, w0=72.62673267326743, w1=15.974286165514334\n",
      "SubGD iter. 315/499: loss=5.310579756254566, w0=72.62673267326743, w1=15.972676504793247\n",
      "SubGD iter. 316/499: loss=5.310576054815085, w0=72.62673267326743, w1=15.97106684407216\n",
      "SubGD iter. 317/499: loss=5.310580617324651, w0=72.63366336633673, w1=15.975535543879564\n",
      "SubGD iter. 318/499: loss=5.3106243531150055, w0=72.62673267326743, w1=15.973409444093827\n",
      "SubGD iter. 319/499: loss=5.310577740220231, w0=72.62673267326743, w1=15.97179978337274\n",
      "SubGD iter. 320/499: loss=5.310575938345063, w0=72.63366336633673, w1=15.976268483180144\n",
      "SubGD iter. 321/499: loss=5.310626579260847, w0=72.62673267326743, w1=15.974142383394407\n",
      "SubGD iter. 322/499: loss=5.31057942562538, w0=72.62673267326743, w1=15.97253272267332\n",
      "SubGD iter. 323/499: loss=5.310575724185897, w0=72.62673267326743, w1=15.970923061952233\n",
      "SubGD iter. 324/499: loss=5.310581535209124, w0=72.63366336633673, w1=15.975391761759637\n",
      "SubGD iter. 325/499: loss=5.310623916407671, w0=72.62673267326743, w1=15.9732656619739\n",
      "SubGD iter. 326/499: loss=5.310577409591045, w0=72.62673267326743, w1=15.971656001252812\n",
      "SubGD iter. 327/499: loss=5.310576856229536, w0=72.63366336633673, w1=15.976124701060217\n",
      "SubGD iter. 328/499: loss=5.310626142553512, w0=72.62673267326743, w1=15.973998601274479\n",
      "SubGD iter. 329/499: loss=5.310579094996192, w0=72.62673267326743, w1=15.972388940553392\n",
      "SubGD iter. 330/499: loss=5.31057539355671, w0=72.62673267326743, w1=15.970779279832305\n",
      "SubGD iter. 331/499: loss=5.310582453093599, w0=72.63366336633673, w1=15.97524797963971\n",
      "SubGD iter. 332/499: loss=5.310623479700335, w0=72.62673267326743, w1=15.973121879853972\n",
      "SubGD iter. 333/499: loss=5.310577078961858, w0=72.62673267326743, w1=15.971512219132885\n",
      "SubGD iter. 334/499: loss=5.3105777741140106, w0=72.63366336633673, w1=15.975980918940289\n",
      "SubGD iter. 335/499: loss=5.310625705846178, w0=72.62673267326743, w1=15.973854819154552\n",
      "SubGD iter. 336/499: loss=5.310578764367005, w0=72.62673267326743, w1=15.972245158433465\n",
      "SubGD iter. 337/499: loss=5.310575062927524, w0=72.62673267326743, w1=15.970635497712378\n",
      "SubGD iter. 338/499: loss=5.3105833709780725, w0=72.63366336633673, w1=15.975104197519782\n",
      "SubGD iter. 339/499: loss=5.310623042993, w0=72.62673267326743, w1=15.972978097734044\n",
      "SubGD iter. 340/499: loss=5.31057674833267, w0=72.62673267326743, w1=15.971368437012957\n",
      "SubGD iter. 341/499: loss=5.310578691998486, w0=72.63366336633673, w1=15.975837136820362\n",
      "SubGD iter. 342/499: loss=5.310625269138844, w0=72.62673267326743, w1=15.973711037034624\n",
      "SubGD iter. 343/499: loss=5.310578433737819, w0=72.62673267326743, w1=15.972101376313537\n",
      "SubGD iter. 344/499: loss=5.3105747322983365, w0=72.62673267326743, w1=15.97049171559245\n",
      "SubGD iter. 345/499: loss=5.310584288862546, w0=72.63366336633673, w1=15.974960415399854\n",
      "SubGD iter. 346/499: loss=5.3106226062856665, w0=72.62673267326743, w1=15.972834315614117\n",
      "SubGD iter. 347/499: loss=5.310576417703483, w0=72.62673267326743, w1=15.97122465489303\n",
      "SubGD iter. 348/499: loss=5.310579609882959, w0=72.63366336633673, w1=15.975693354700434\n",
      "SubGD iter. 349/499: loss=5.310624832431509, w0=72.62673267326743, w1=15.973567254914697\n",
      "SubGD iter. 350/499: loss=5.3105781031086305, w0=72.62673267326743, w1=15.97195759419361\n",
      "SubGD iter. 351/499: loss=5.310574930903372, w0=72.63366336633673, w1=15.976426294001014\n",
      "SubGD iter. 352/499: loss=5.310627058577351, w0=72.62673267326743, w1=15.974300194215276\n",
      "SubGD iter. 353/499: loss=5.3105797885137775, w0=72.62673267326743, w1=15.97269053349419\n",
      "SubGD iter. 354/499: loss=5.310576087074297, w0=72.62673267326743, w1=15.971080872773102\n",
      "SubGD iter. 355/499: loss=5.310580527767432, w0=72.63366336633673, w1=15.975549572580507\n",
      "SubGD iter. 356/499: loss=5.310624395724173, w0=72.62673267326743, w1=15.973423472794769\n",
      "SubGD iter. 357/499: loss=5.310577772479444, w0=72.62673267326743, w1=15.971813812073682\n",
      "SubGD iter. 358/499: loss=5.3105758487878445, w0=72.63366336633673, w1=15.976282511881086\n",
      "SubGD iter. 359/499: loss=5.310626621870017, w0=72.62673267326743, w1=15.974156412095349\n",
      "SubGD iter. 360/499: loss=5.31057945788459, w0=72.62673267326743, w1=15.972546751374262\n",
      "SubGD iter. 361/499: loss=5.31057575644511, w0=72.62673267326743, w1=15.970937090653175\n",
      "SubGD iter. 362/499: loss=5.310581445651906, w0=72.63366336633673, w1=15.975405790460579\n",
      "SubGD iter. 363/499: loss=5.310623959016839, w0=72.62673267326743, w1=15.973279690674842\n",
      "SubGD iter. 364/499: loss=5.310577441850257, w0=72.62673267326743, w1=15.971670029953755\n",
      "SubGD iter. 365/499: loss=5.310576766672319, w0=72.63366336633673, w1=15.976138729761159\n",
      "SubGD iter. 366/499: loss=5.310626185162682, w0=72.62673267326743, w1=15.974012629975421\n",
      "SubGD iter. 367/499: loss=5.310579127255404, w0=72.62673267326743, w1=15.972402969254334\n",
      "SubGD iter. 368/499: loss=5.310575425815924, w0=72.62673267326743, w1=15.970793308533247\n",
      "SubGD iter. 369/499: loss=5.310582363536381, w0=72.63366336633673, w1=15.975262008340652\n",
      "SubGD iter. 370/499: loss=5.310623522309504, w0=72.62673267326743, w1=15.973135908554914\n",
      "SubGD iter. 371/499: loss=5.31057711122107, w0=72.62673267326743, w1=15.971526247833827\n",
      "SubGD iter. 372/499: loss=5.310577684556793, w0=72.63366336633673, w1=15.975994947641231\n",
      "SubGD iter. 373/499: loss=5.310625748455347, w0=72.62673267326743, w1=15.973868847855494\n",
      "SubGD iter. 374/499: loss=5.310578796626218, w0=72.62673267326743, w1=15.972259187134407\n",
      "SubGD iter. 375/499: loss=5.310575095186736, w0=72.62673267326743, w1=15.97064952641332\n",
      "SubGD iter. 376/499: loss=5.310583281420854, w0=72.63366336633673, w1=15.975118226220724\n",
      "SubGD iter. 377/499: loss=5.3106230856021694, w0=72.62673267326743, w1=15.972992126434987\n",
      "SubGD iter. 378/499: loss=5.310576780591885, w0=72.62673267326743, w1=15.9713824657139\n",
      "SubGD iter. 379/499: loss=5.310578602441266, w0=72.63366336633673, w1=15.975851165521304\n",
      "SubGD iter. 380/499: loss=5.310625311748012, w0=72.62673267326743, w1=15.973725065735566\n",
      "SubGD iter. 381/499: loss=5.310578465997031, w0=72.62673267326743, w1=15.97211540501448\n",
      "SubGD iter. 382/499: loss=5.31057476455755, w0=72.62673267326743, w1=15.970505744293392\n",
      "SubGD iter. 383/499: loss=5.310584199305326, w0=72.63366336633673, w1=15.974974444100797\n",
      "SubGD iter. 384/499: loss=5.310622648894835, w0=72.62673267326743, w1=15.972848344315059\n",
      "SubGD iter. 385/499: loss=5.310576449962697, w0=72.62673267326743, w1=15.971238683593972\n",
      "SubGD iter. 386/499: loss=5.31057952032574, w0=72.63366336633673, w1=15.975707383401376\n",
      "SubGD iter. 387/499: loss=5.310624875040677, w0=72.62673267326743, w1=15.973581283615639\n",
      "SubGD iter. 388/499: loss=5.3105781353678445, w0=72.62673267326743, w1=15.971971622894552\n",
      "SubGD iter. 389/499: loss=5.310574841346153, w0=72.63366336633673, w1=15.976440322701956\n",
      "SubGD iter. 390/499: loss=5.31062710118652, w0=72.62673267326743, w1=15.974314222916218\n",
      "SubGD iter. 391/499: loss=5.3105798207729915, w0=72.62673267326743, w1=15.972704562195132\n",
      "SubGD iter. 392/499: loss=5.3105761193335095, w0=72.62673267326743, w1=15.971094901474045\n",
      "SubGD iter. 393/499: loss=5.310580438210215, w0=72.63366336633673, w1=15.975563601281449\n",
      "SubGD iter. 394/499: loss=5.310624438333343, w0=72.62673267326743, w1=15.973437501495711\n",
      "SubGD iter. 395/499: loss=5.3105778047386565, w0=72.62673267326743, w1=15.971827840774624\n",
      "SubGD iter. 396/499: loss=5.310575759230627, w0=72.63366336633673, w1=15.976296540582029\n",
      "SubGD iter. 397/499: loss=5.310626664479185, w0=72.62673267326743, w1=15.974170440796291\n",
      "SubGD iter. 398/499: loss=5.3105794901438035, w0=72.62673267326743, w1=15.972560780075204\n",
      "SubGD iter. 399/499: loss=5.310575788704323, w0=72.62673267326743, w1=15.970951119354117\n",
      "SubGD iter. 400/499: loss=5.310581356094687, w0=72.63366336633673, w1=15.975419819161521\n",
      "SubGD iter. 401/499: loss=5.310624001626008, w0=72.62673267326743, w1=15.973293719375784\n",
      "SubGD iter. 402/499: loss=5.31057747410947, w0=72.62673267326743, w1=15.971684058654697\n",
      "SubGD iter. 403/499: loss=5.310576677115099, w0=72.63366336633673, w1=15.976152758462101\n",
      "SubGD iter. 404/499: loss=5.31062622777185, w0=72.62673267326743, w1=15.974026658676364\n",
      "SubGD iter. 405/499: loss=5.310579159514617, w0=72.62673267326743, w1=15.972416997955277\n",
      "SubGD iter. 406/499: loss=5.310575458075135, w0=72.62673267326743, w1=15.97080733723419\n",
      "SubGD iter. 407/499: loss=5.310582273979162, w0=72.63366336633673, w1=15.975276037041594\n",
      "SubGD iter. 408/499: loss=5.310623564918673, w0=72.62673267326743, w1=15.973149937255856\n",
      "SubGD iter. 409/499: loss=5.310577143480284, w0=72.62673267326743, w1=15.97154027653477\n",
      "SubGD iter. 410/499: loss=5.3105775949995735, w0=72.63366336633673, w1=15.976008976342174\n",
      "SubGD iter. 411/499: loss=5.310625791064514, w0=72.62673267326743, w1=15.973882876556436\n",
      "SubGD iter. 412/499: loss=5.31057882888543, w0=72.62673267326743, w1=15.972273215835349\n",
      "SubGD iter. 413/499: loss=5.310575127445949, w0=72.62673267326743, w1=15.970663555114262\n",
      "SubGD iter. 414/499: loss=5.310583191863635, w0=72.63366336633673, w1=15.975132254921666\n",
      "SubGD iter. 415/499: loss=5.310623128211338, w0=72.62673267326743, w1=15.973006155135929\n",
      "SubGD iter. 416/499: loss=5.310576812851096, w0=72.62673267326743, w1=15.971396494414842\n",
      "SubGD iter. 417/499: loss=5.310578512884048, w0=72.63366336633673, w1=15.975865194222246\n",
      "SubGD iter. 418/499: loss=5.31062535435718, w0=72.62673267326743, w1=15.973739094436509\n",
      "SubGD iter. 419/499: loss=5.310578498256244, w0=72.62673267326743, w1=15.972129433715422\n",
      "SubGD iter. 420/499: loss=5.310574796816762, w0=72.62673267326743, w1=15.970519772994335\n",
      "SubGD iter. 421/499: loss=5.31058410974811, w0=72.63366336633673, w1=15.974988472801739\n",
      "SubGD iter. 422/499: loss=5.310622691504003, w0=72.62673267326743, w1=15.972862373016001\n",
      "SubGD iter. 423/499: loss=5.310576482221909, w0=72.62673267326743, w1=15.971252712294914\n",
      "SubGD iter. 424/499: loss=5.310579430768521, w0=72.63366336633673, w1=15.975721412102319\n",
      "SubGD iter. 425/499: loss=5.310624917649847, w0=72.62673267326743, w1=15.973595312316581\n",
      "SubGD iter. 426/499: loss=5.310578167627056, w0=72.62673267326743, w1=15.971985651595494\n",
      "SubGD iter. 427/499: loss=5.310574751788934, w0=72.63366336633673, w1=15.976454351402898\n",
      "SubGD iter. 428/499: loss=5.310627143795689, w0=72.62673267326743, w1=15.97432825161716\n",
      "SubGD iter. 429/499: loss=5.310579853032204, w0=72.62673267326743, w1=15.972718590896074\n",
      "SubGD iter. 430/499: loss=5.310576151592722, w0=72.62673267326743, w1=15.971108930174987\n",
      "SubGD iter. 431/499: loss=5.310580348652994, w0=72.63366336633673, w1=15.975577629982391\n",
      "SubGD iter. 432/499: loss=5.310624480942511, w0=72.62673267326743, w1=15.973451530196654\n",
      "SubGD iter. 433/499: loss=5.31057783699787, w0=72.62673267326743, w1=15.971841869475567\n",
      "SubGD iter. 434/499: loss=5.3105756696734066, w0=72.63366336633673, w1=15.97631056928297\n",
      "SubGD iter. 435/499: loss=5.310626707088354, w0=72.62673267326743, w1=15.974184469497233\n",
      "SubGD iter. 436/499: loss=5.310579522403018, w0=72.62673267326743, w1=15.972574808776146\n",
      "SubGD iter. 437/499: loss=5.310575820963536, w0=72.62673267326743, w1=15.97096514805506\n",
      "SubGD iter. 438/499: loss=5.3105812665374685, w0=72.63366336633673, w1=15.975433847862464\n",
      "SubGD iter. 439/499: loss=5.310624044235177, w0=72.62673267326743, w1=15.973307748076726\n",
      "SubGD iter. 440/499: loss=5.310577506368682, w0=72.62673267326743, w1=15.97169808735564\n",
      "SubGD iter. 441/499: loss=5.31057658755788, w0=72.63366336633673, w1=15.976166787163043\n",
      "SubGD iter. 442/499: loss=5.310626270381019, w0=72.62673267326743, w1=15.974040687377306\n",
      "SubGD iter. 443/499: loss=5.31057919177383, w0=72.62673267326743, w1=15.972431026656219\n",
      "SubGD iter. 444/499: loss=5.310575490334348, w0=72.62673267326743, w1=15.970821365935132\n",
      "SubGD iter. 445/499: loss=5.310582184421943, w0=72.63366336633673, w1=15.975290065742536\n",
      "SubGD iter. 446/499: loss=5.310623607527842, w0=72.62673267326743, w1=15.973163965956799\n",
      "SubGD iter. 447/499: loss=5.310577175739496, w0=72.62673267326743, w1=15.971554305235712\n",
      "SubGD iter. 448/499: loss=5.310577505442355, w0=72.63366336633673, w1=15.976023005043116\n",
      "SubGD iter. 449/499: loss=5.310625833673684, w0=72.62673267326743, w1=15.973896905257378\n",
      "SubGD iter. 450/499: loss=5.310578861144643, w0=72.62673267326743, w1=15.972287244536291\n",
      "SubGD iter. 451/499: loss=5.310575159705162, w0=72.62673267326743, w1=15.970677583815204\n",
      "SubGD iter. 452/499: loss=5.310583102306416, w0=72.63366336633673, w1=15.975146283622609\n",
      "SubGD iter. 453/499: loss=5.310623170820506, w0=72.62673267326743, w1=15.973020183836871\n",
      "SubGD iter. 454/499: loss=5.310576845110308, w0=72.62673267326743, w1=15.971410523115784\n",
      "SubGD iter. 455/499: loss=5.310578423326827, w0=72.63366336633673, w1=15.975879222923188\n",
      "SubGD iter. 456/499: loss=5.3106253969663495, w0=72.62673267326743, w1=15.97375312313745\n",
      "SubGD iter. 457/499: loss=5.310578530515456, w0=72.62673267326743, w1=15.972143462416364\n",
      "SubGD iter. 458/499: loss=5.310574829075975, w0=72.62673267326743, w1=15.970533801695277\n",
      "SubGD iter. 459/499: loss=5.3105840201908885, w0=72.63366336633673, w1=15.975002501502681\n",
      "SubGD iter. 460/499: loss=5.3106227341131715, w0=72.62673267326743, w1=15.972876401716944\n",
      "SubGD iter. 461/499: loss=5.310576514481122, w0=72.62673267326743, w1=15.971266740995857\n",
      "SubGD iter. 462/499: loss=5.3105793412113025, w0=72.63366336633673, w1=15.97573544080326\n",
      "SubGD iter. 463/499: loss=5.310624960259014, w0=72.62673267326743, w1=15.973609341017523\n",
      "SubGD iter. 464/499: loss=5.310578199886269, w0=72.62673267326743, w1=15.971999680296436\n",
      "SubGD iter. 465/499: loss=5.310574662231715, w0=72.63366336633673, w1=15.97646838010384\n",
      "SubGD iter. 466/499: loss=5.310627186404858, w0=72.62673267326743, w1=15.974342280318103\n",
      "SubGD iter. 467/499: loss=5.310579885291417, w0=72.62673267326743, w1=15.972732619597016\n",
      "SubGD iter. 468/499: loss=5.310576183851936, w0=72.62673267326743, w1=15.97112295887593\n",
      "SubGD iter. 469/499: loss=5.310580259095775, w0=72.63366336633673, w1=15.975591658683333\n",
      "SubGD iter. 470/499: loss=5.310624523551678, w0=72.62673267326743, w1=15.973465558897596\n",
      "SubGD iter. 471/499: loss=5.310577869257083, w0=72.62673267326743, w1=15.971855898176509\n",
      "SubGD iter. 472/499: loss=5.310575580116187, w0=72.63366336633673, w1=15.976324597983913\n",
      "SubGD iter. 473/499: loss=5.310626749697523, w0=72.62673267326743, w1=15.974198498198175\n",
      "SubGD iter. 474/499: loss=5.3105795546622305, w0=72.62673267326743, w1=15.972588837477089\n",
      "SubGD iter. 475/499: loss=5.3105758532227485, w0=72.62673267326743, w1=15.970979176756002\n",
      "SubGD iter. 476/499: loss=5.310581176980249, w0=72.63366336633673, w1=15.975447876563406\n",
      "SubGD iter. 477/499: loss=5.310624086844345, w0=72.62673267326743, w1=15.973321776777668\n",
      "SubGD iter. 478/499: loss=5.3105775386278955, w0=72.62673267326743, w1=15.971712116056581\n",
      "SubGD iter. 479/499: loss=5.3105764980006605, w0=72.63366336633673, w1=15.976180815863986\n",
      "SubGD iter. 480/499: loss=5.310626312990188, w0=72.62673267326743, w1=15.974054716078248\n",
      "SubGD iter. 481/499: loss=5.3105792240330425, w0=72.62673267326743, w1=15.972445055357161\n",
      "SubGD iter. 482/499: loss=5.3105755225935605, w0=72.62673267326743, w1=15.970835394636074\n",
      "SubGD iter. 483/499: loss=5.310582094864723, w0=72.63366336633673, w1=15.975304094443478\n",
      "SubGD iter. 484/499: loss=5.31062365013701, w0=72.62673267326743, w1=15.97317799465774\n",
      "SubGD iter. 485/499: loss=5.3105772079987075, w0=72.62673267326743, w1=15.971568333936654\n",
      "SubGD iter. 486/499: loss=5.3105774158851355, w0=72.63366336633673, w1=15.976037033744058\n",
      "SubGD iter. 487/499: loss=5.310625876282853, w0=72.62673267326743, w1=15.97391093395832\n",
      "SubGD iter. 488/499: loss=5.310578893403856, w0=72.62673267326743, w1=15.972301273237234\n",
      "SubGD iter. 489/499: loss=5.310575191964374, w0=72.62673267326743, w1=15.970691612516147\n",
      "SubGD iter. 490/499: loss=5.3105830127491975, w0=72.63366336633673, w1=15.97516031232355\n",
      "SubGD iter. 491/499: loss=5.310623213429675, w0=72.62673267326743, w1=15.973034212537813\n",
      "SubGD iter. 492/499: loss=5.310576877369521, w0=72.62673267326743, w1=15.971424551816726\n",
      "SubGD iter. 493/499: loss=5.310578333769609, w0=72.63366336633673, w1=15.97589325162413\n",
      "SubGD iter. 494/499: loss=5.310625439575518, w0=72.62673267326743, w1=15.973767151838393\n",
      "SubGD iter. 495/499: loss=5.310578562774669, w0=72.62673267326743, w1=15.972157491117306\n",
      "SubGD iter. 496/499: loss=5.310574861335188, w0=72.62673267326743, w1=15.97054783039622\n",
      "SubGD iter. 497/499: loss=5.310583930633672, w0=72.63366336633673, w1=15.975016530203623\n",
      "SubGD iter. 498/499: loss=5.310622776722341, w0=72.62673267326743, w1=15.972890430417886\n",
      "SubGD iter. 499/499: loss=5.310576546740335, w0=72.62673267326743, w1=15.971280769696799\n",
      "SubGD: execution time=0.066 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(\n",
    "    y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84cc53cbdeaf4a70b605faf42ca6f981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses, subgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic SubGradient Descent algorithm (SubSGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic subgradient descent.\n",
    "        # ***************************************************\n",
    "        loss = compute_loss_mae(y, tx, w)\n",
    "        grad = compute_subgradient_mae(y, tx, w)\n",
    "        w = w - gamma*grad\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        print(\"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=74.06780585492638, w0=0.7, w1=6.109524327590712e-16\n",
      "SubSGD iter. 1/499: loss=73.36780585492637, w0=1.4, w1=1.2219048655181425e-15\n",
      "SubSGD iter. 2/499: loss=72.66780585492637, w0=2.0999999999999996, w1=1.832857298277214e-15\n",
      "SubSGD iter. 3/499: loss=71.96780585492638, w0=2.8, w1=2.443809731036285e-15\n",
      "SubSGD iter. 4/499: loss=71.26780585492638, w0=3.5, w1=3.054762163795356e-15\n",
      "SubSGD iter. 5/499: loss=70.56780585492638, w0=4.2, w1=3.665714596554428e-15\n",
      "SubSGD iter. 6/499: loss=69.86780585492637, w0=4.9, w1=4.276667029313499e-15\n",
      "SubSGD iter. 7/499: loss=69.16780585492639, w0=5.6000000000000005, w1=4.887619462072571e-15\n",
      "SubSGD iter. 8/499: loss=68.46780585492637, w0=6.300000000000001, w1=5.498571894831642e-15\n",
      "SubSGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000001, w1=6.109524327590714e-15\n",
      "SubSGD iter. 10/499: loss=67.06780585492638, w0=7.700000000000001, w1=6.720476760349785e-15\n",
      "SubSGD iter. 11/499: loss=66.36780585492637, w0=8.4, w1=7.331429193108857e-15\n",
      "SubSGD iter. 12/499: loss=65.66780585492639, w0=9.1, w1=7.942381625867928e-15\n",
      "SubSGD iter. 13/499: loss=64.96780585492638, w0=9.799999999999999, w1=8.553334058627e-15\n",
      "SubSGD iter. 14/499: loss=64.26780585492638, w0=10.499999999999998, w1=9.164286491386072e-15\n",
      "SubSGD iter. 15/499: loss=63.567805854926384, w0=11.199999999999998, w1=9.775238924145143e-15\n",
      "SubSGD iter. 16/499: loss=62.867805854926374, w0=11.899999999999997, w1=1.0386191356904215e-14\n",
      "SubSGD iter. 17/499: loss=62.167805854926385, w0=12.599999999999996, w1=1.0997143789663286e-14\n",
      "SubSGD iter. 18/499: loss=61.46780585492638, w0=13.299999999999995, w1=1.1608096222422358e-14\n",
      "SubSGD iter. 19/499: loss=60.76780585492638, w0=13.999999999999995, w1=1.2219048655181429e-14\n",
      "SubSGD iter. 20/499: loss=60.067805854926384, w0=14.699999999999994, w1=1.28300010879405e-14\n",
      "SubSGD iter. 21/499: loss=59.36780585492639, w0=15.399999999999993, w1=1.3440953520699572e-14\n",
      "SubSGD iter. 22/499: loss=58.667805854926385, w0=16.099999999999994, w1=1.4051905953458644e-14\n",
      "SubSGD iter. 23/499: loss=57.96780585492638, w0=16.799999999999994, w1=1.4662858386217714e-14\n",
      "SubSGD iter. 24/499: loss=57.26780585492638, w0=17.499999999999993, w1=1.5273810818976784e-14\n",
      "SubSGD iter. 25/499: loss=56.567805854926384, w0=18.199999999999992, w1=1.5884763251735854e-14\n",
      "SubSGD iter. 26/499: loss=55.867805854926395, w0=18.89999999999999, w1=1.6495715684494924e-14\n",
      "SubSGD iter. 27/499: loss=55.167805854926385, w0=19.59999999999999, w1=1.7106668117253994e-14\n",
      "SubSGD iter. 28/499: loss=54.46780585492638, w0=20.29999999999999, w1=1.7717620550013064e-14\n",
      "SubSGD iter. 29/499: loss=53.767805854926394, w0=20.99999999999999, w1=1.8328572982772134e-14\n",
      "SubSGD iter. 30/499: loss=53.06780585492639, w0=21.69999999999999, w1=1.8939525415531204e-14\n",
      "SubSGD iter. 31/499: loss=52.367805854926395, w0=22.399999999999988, w1=1.9550477848290273e-14\n",
      "SubSGD iter. 32/499: loss=51.667805854926385, w0=23.099999999999987, w1=2.0161430281049343e-14\n",
      "SubSGD iter. 33/499: loss=50.96780585492639, w0=23.799999999999986, w1=2.0772382713808413e-14\n",
      "SubSGD iter. 34/499: loss=50.267805854926394, w0=24.499999999999986, w1=2.1383335146567483e-14\n",
      "SubSGD iter. 35/499: loss=49.56780585492639, w0=25.199999999999985, w1=2.1994287579326553e-14\n",
      "SubSGD iter. 36/499: loss=48.867805854926395, w0=25.899999999999984, w1=2.2605240012085623e-14\n",
      "SubSGD iter. 37/499: loss=48.1678058549264, w0=26.599999999999984, w1=2.3216192444844693e-14\n",
      "SubSGD iter. 38/499: loss=47.4678058549264, w0=27.299999999999983, w1=2.3827144877603763e-14\n",
      "SubSGD iter. 39/499: loss=46.7678058549264, w0=27.999999999999982, w1=2.4438097310362833e-14\n",
      "SubSGD iter. 40/499: loss=46.067805854926405, w0=28.69999999999998, w1=2.5049049743121903e-14\n",
      "SubSGD iter. 41/499: loss=45.367805854926395, w0=29.39999999999998, w1=2.5660002175880973e-14\n",
      "SubSGD iter. 42/499: loss=44.6678058549264, w0=30.09999999999998, w1=2.6270954608640043e-14\n",
      "SubSGD iter. 43/499: loss=43.9678058549264, w0=30.79999999999998, w1=2.6881907041399113e-14\n",
      "SubSGD iter. 44/499: loss=43.2678058549264, w0=31.49999999999998, w1=2.7492859474158183e-14\n",
      "SubSGD iter. 45/499: loss=42.567805854926405, w0=32.19999999999998, w1=2.8103811906917253e-14\n",
      "SubSGD iter. 46/499: loss=41.867805854926395, w0=32.899999999999984, w1=2.871476433967632e-14\n",
      "SubSGD iter. 47/499: loss=41.167805854926385, w0=33.59999999999999, w1=2.9325716772435396e-14\n",
      "SubSGD iter. 48/499: loss=40.46780585492639, w0=34.29999999999999, w1=2.993666920519447e-14\n",
      "SubSGD iter. 49/499: loss=39.767805854926394, w0=34.99999999999999, w1=3.054762163795354e-14\n",
      "SubSGD iter. 50/499: loss=39.067805854926384, w0=35.699999999999996, w1=3.1158574070712615e-14\n",
      "SubSGD iter. 51/499: loss=38.36780585492638, w0=36.4, w1=3.176952650347169e-14\n",
      "SubSGD iter. 52/499: loss=37.66780585492638, w0=37.1, w1=3.238047893623076e-14\n",
      "SubSGD iter. 53/499: loss=36.96780585492638, w0=37.800000000000004, w1=3.2991431368989835e-14\n",
      "SubSGD iter. 54/499: loss=36.26780585492637, w0=38.50000000000001, w1=3.360238380174891e-14\n",
      "SubSGD iter. 55/499: loss=35.56780585492637, w0=39.20000000000001, w1=3.421333623450798e-14\n",
      "SubSGD iter. 56/499: loss=34.86780585492637, w0=39.90000000000001, w1=3.4824288667267054e-14\n",
      "SubSGD iter. 57/499: loss=34.16780585492637, w0=40.600000000000016, w1=3.543524110002613e-14\n",
      "SubSGD iter. 58/499: loss=33.46780585492636, w0=41.30000000000002, w1=3.60461935327852e-14\n",
      "SubSGD iter. 59/499: loss=32.767805854926365, w0=42.00000000000002, w1=3.6657145965544273e-14\n",
      "SubSGD iter. 60/499: loss=32.067805854926355, w0=42.700000000000024, w1=3.7268098398303347e-14\n",
      "SubSGD iter. 61/499: loss=31.36780585492636, w0=43.40000000000003, w1=3.787905083106242e-14\n",
      "SubSGD iter. 62/499: loss=30.66780585492635, w0=44.10000000000003, w1=3.849000326382149e-14\n",
      "SubSGD iter. 63/499: loss=29.967805854926347, w0=44.80000000000003, w1=3.9100955696580566e-14\n",
      "SubSGD iter. 64/499: loss=29.267805854926348, w0=45.500000000000036, w1=3.971190812933964e-14\n",
      "SubSGD iter. 65/499: loss=28.567805854926345, w0=46.20000000000004, w1=4.032286056209871e-14\n",
      "SubSGD iter. 66/499: loss=27.867805854926342, w0=46.90000000000004, w1=4.0933812994857785e-14\n",
      "SubSGD iter. 67/499: loss=27.17327020966892, w0=47.59306930693074, w1=0.011147845678271063\n",
      "SubSGD iter. 68/499: loss=26.490451563751197, w0=48.279207920792125, w1=0.03308574108989941\n",
      "SubSGD iter. 69/499: loss=25.81721232277017, w0=48.96534653465351, w1=0.05502363650152776\n",
      "SubSGD iter. 70/499: loss=25.15503943465645, w0=49.63069306930698, w1=0.10538326388307814\n",
      "SubSGD iter. 71/499: loss=24.524103413894778, w0=50.28910891089114, w1=0.16746568532793435\n",
      "SubSGD iter. 72/499: loss=23.899295346035593, w0=50.947524752475296, w1=0.22954810677279056\n",
      "SubSGD iter. 73/499: loss=23.284392925657144, w0=51.59207920792084, w1=0.31242512932747524\n",
      "SubSGD iter. 74/499: loss=22.686876444181845, w0=52.22277227722777, w1=0.4119501328839991\n",
      "SubSGD iter. 75/499: loss=22.10626756964055, w0=52.84653465346539, w1=0.5208167847923756\n",
      "SubSGD iter. 76/499: loss=21.537818828008433, w0=53.4564356435644, w1=0.6457900912635992\n",
      "SubSGD iter. 77/499: loss=20.986339874628463, w0=54.0594059405941, w1=0.7796904498577214\n",
      "SubSGD iter. 78/499: loss=20.445560936620446, w0=54.655445544554496, w1=0.9197570104995693\n",
      "SubSGD iter. 79/499: loss=19.91191015895785, w0=55.24455445544559, w1=1.0670920297849913\n",
      "SubSGD iter. 80/499: loss=19.389644090563234, w0=55.819801980198065, w1=1.2261255948210765\n",
      "SubSGD iter. 81/499: loss=18.887989064395885, w0=56.36732673267331, w1=1.410709342622213\n",
      "SubSGD iter. 82/499: loss=18.415960501854236, w0=56.900990099009945, w1=1.605853732220269\n",
      "SubSGD iter. 83/499: loss=17.954898543040386, w0=57.42772277227727, w1=1.808762802293962\n",
      "SubSGD iter. 84/499: loss=17.505757656579824, w0=57.933663366336674, w1=2.0285064197514697\n",
      "SubSGD iter. 85/499: loss=17.07495742693161, w0=58.43267326732677, w1=2.2494370848672776\n",
      "SubSGD iter. 86/499: loss=16.652967297509903, w0=58.91089108910895, w1=2.4837982986028337\n",
      "SubSGD iter. 87/499: loss=16.24854073149673, w0=59.382178217821824, w1=2.7260245553531504\n",
      "SubSGD iter. 88/499: loss=15.849105212654159, w0=59.83960396039608, w1=2.978742333469136\n",
      "SubSGD iter. 89/499: loss=15.46691979123133, w0=60.262376237623805, w1=3.251528669355438\n",
      "SubSGD iter. 90/499: loss=15.108294621512215, w0=60.67821782178222, w1=3.5270865794242794\n",
      "SubSGD iter. 91/499: loss=14.754896345922832, w0=61.087128712871326, w1=3.806459183951815\n",
      "SubSGD iter. 92/499: loss=14.40452896162028, w0=61.49603960396043, w1=4.085831788479351\n",
      "SubSGD iter. 93/499: loss=14.055787028127279, w0=61.891089108910926, w1=4.373839384328607\n",
      "SubSGD iter. 94/499: loss=13.714620911605635, w0=62.27920792079211, w1=4.666037469532047\n",
      "SubSGD iter. 95/499: loss=13.381236307284155, w0=62.65346534653469, w1=4.959829093241769\n",
      "SubSGD iter. 96/499: loss=13.058821615166238, w0=63.02079207920796, w1=5.25705719205664\n",
      "SubSGD iter. 97/499: loss=12.74025172433924, w0=63.38118811881192, w1=5.560434316352406\n",
      "SubSGD iter. 98/499: loss=12.42321888875611, w0=63.74158415841588, w1=5.863811440648173\n",
      "SubSGD iter. 99/499: loss=12.107561731901173, w0=64.08811881188123, w1=6.172402175278548\n",
      "SubSGD iter. 100/499: loss=11.800622097398135, w0=64.42772277227726, w1=6.486369310516498\n",
      "SubSGD iter. 101/499: loss=11.495041794646427, w0=64.7673267326733, w1=6.800336445754448\n",
      "SubSGD iter. 102/499: loss=11.189461491894715, w0=65.10693069306933, w1=7.114303580992399\n",
      "SubSGD iter. 103/499: loss=10.883881189143004, w0=65.44653465346536, w1=7.428270716230349\n",
      "SubSGD iter. 104/499: loss=10.584593408313202, w0=65.76534653465349, w1=7.747893210218626\n",
      "SubSGD iter. 105/499: loss=10.295816534318941, w0=66.070297029703, w1=8.073669686866905\n",
      "SubSGD iter. 106/499: loss=10.01135208122136, w0=66.37524752475251, w1=8.399446163515185\n",
      "SubSGD iter. 107/499: loss=9.72808432666813, w0=66.6663366336634, w1=8.73297028041739\n",
      "SubSGD iter. 108/499: loss=9.44812546112251, w0=66.9574257425743, w1=9.066494397319596\n",
      "SubSGD iter. 109/499: loss=9.17104110409667, w0=67.23465346534658, w1=9.39863031947029\n",
      "SubSGD iter. 110/499: loss=8.903656131158964, w0=67.51188118811886, w1=9.730766241620982\n",
      "SubSGD iter. 111/499: loss=8.636271158221257, w0=67.78910891089114, w1=10.062902163771675\n",
      "SubSGD iter. 112/499: loss=8.376151920302375, w0=68.06633663366343, w1=10.363999289979422\n",
      "SubSGD iter. 113/499: loss=8.140540838751496, w0=68.32970297029709, w1=10.660466909273612\n",
      "SubSGD iter. 114/499: loss=7.918544501597273, w0=68.59306930693076, w1=10.943174379960814\n",
      "SubSGD iter. 115/499: loss=7.705279728377, w0=68.85643564356442, w1=11.225881850648015\n",
      "SubSGD iter. 116/499: loss=7.493695831178641, w0=69.11287128712878, w1=11.504395843582206\n",
      "SubSGD iter. 117/499: loss=7.289992405743416, w0=69.35544554455453, w1=11.78820189306775\n",
      "SubSGD iter. 118/499: loss=7.097234035781543, w0=69.58415841584166, w1=12.060911465190971\n",
      "SubSGD iter. 119/499: loss=6.919905294668923, w0=69.80594059405948, w1=12.324245668386048\n",
      "SubSGD iter. 120/499: loss=6.750573527315454, w0=70.0277227722773, w1=12.587579871581125\n",
      "SubSGD iter. 121/499: loss=6.584744810805664, w0=70.25643564356443, w1=12.824765405096484\n",
      "SubSGD iter. 122/499: loss=6.430343276347806, w0=70.47821782178225, w1=13.065616959310148\n",
      "SubSGD iter. 123/499: loss=6.278071481890353, w0=70.69306930693077, w1=13.302953389983912\n",
      "SubSGD iter. 124/499: loss=6.133663329263324, w0=70.89405940594067, w1=13.525403099312918\n",
      "SubSGD iter. 125/499: loss=6.00584079834303, w0=71.08811881188126, w1=13.742945617944212\n",
      "SubSGD iter. 126/499: loss=5.885021825223219, w0=71.27524752475254, w1=13.953548196006844\n",
      "SubSGD iter. 127/499: loss=5.771635252269658, w0=71.46237623762383, w1=14.164150774069476\n",
      "SubSGD iter. 128/499: loss=5.667162061790257, w0=71.62178217821788, w1=14.349779559473173\n",
      "SubSGD iter. 129/499: loss=5.586726765993146, w0=71.75346534653471, w1=14.51689010761231\n",
      "SubSGD iter. 130/499: loss=5.523847812160388, w0=71.87128712871292, w1=14.670791185324186\n",
      "SubSGD iter. 131/499: loss=5.480093708591872, w0=71.95445544554461, w1=14.780276456654521\n",
      "SubSGD iter. 132/499: loss=5.4530880035020255, w0=72.0376237623763, w1=14.889761727984856\n",
      "SubSGD iter. 133/499: loss=5.427392630862905, w0=72.10693069306937, w1=14.985916181776727\n",
      "SubSGD iter. 134/499: loss=5.407322445682752, w0=72.17623762376245, w1=15.082070635568597\n",
      "SubSGD iter. 135/499: loss=5.387252260502599, w0=72.24554455445552, w1=15.178225089360467\n",
      "SubSGD iter. 136/499: loss=5.3704607803386955, w0=72.30099009900998, w1=15.25972348971591\n",
      "SubSGD iter. 137/499: loss=5.357406523334741, w0=72.34950495049513, w1=15.335091856448138\n",
      "SubSGD iter. 138/499: loss=5.345929264022584, w0=72.39801980198028, w1=15.410460223180365\n",
      "SubSGD iter. 139/499: loss=5.335714659517473, w0=72.43267326732682, w1=15.469961786755725\n",
      "SubSGD iter. 140/499: loss=5.330043910465361, w0=72.46039603960405, w1=15.51864528583281\n",
      "SubSGD iter. 141/499: loss=5.325676428273225, w0=72.48811881188128, w1=15.561592159086487\n",
      "SubSGD iter. 142/499: loss=5.322176726526591, w0=72.5019801980199, w1=15.597828332032526\n",
      "SubSGD iter. 143/499: loss=5.320111309643114, w0=72.52277227722782, w1=15.624722856626713\n",
      "SubSGD iter. 144/499: loss=5.318478284898438, w0=72.55049504950505, w1=15.642690329098\n",
      "SubSGD iter. 145/499: loss=5.3172400485651465, w0=72.56435643564366, w1=15.664356578291091\n",
      "SubSGD iter. 146/499: loss=5.316406547951545, w0=72.58514851485158, w1=15.677095775361284\n",
      "SubSGD iter. 147/499: loss=5.315557122666144, w0=72.6059405940595, w1=15.689834972431477\n",
      "SubSGD iter. 148/499: loss=5.31470769738074, w0=72.62673267326743, w1=15.70257416950167\n",
      "SubSGD iter. 149/499: loss=5.313876880922167, w0=72.64059405940604, w1=15.72424041869476\n",
      "SubSGD iter. 150/499: loss=5.3130522468713846, w0=72.66138613861396, w1=15.736979615764954\n",
      "SubSGD iter. 151/499: loss=5.312377839024387, w0=72.66831683168327, w1=15.74811029423128\n",
      "SubSGD iter. 152/499: loss=5.312132229725043, w0=72.67524752475258, w1=15.759240972697606\n",
      "SubSGD iter. 153/499: loss=5.311886620425697, w0=72.68217821782189, w1=15.770371651163932\n",
      "SubSGD iter. 154/499: loss=5.311683566098433, w0=72.68217821782189, w1=15.774323911906686\n",
      "SubSGD iter. 155/499: loss=5.311661251291322, w0=72.68217821782189, w1=15.77827617264944\n",
      "SubSGD iter. 156/499: loss=5.311638936484209, w0=72.68217821782189, w1=15.782228433392193\n",
      "SubSGD iter. 157/499: loss=5.311616621677096, w0=72.68217821782189, w1=15.786180694134947\n",
      "SubSGD iter. 158/499: loss=5.311594306869985, w0=72.68217821782189, w1=15.7901329548777\n",
      "SubSGD iter. 159/499: loss=5.311571992062872, w0=72.68217821782189, w1=15.794085215620454\n",
      "SubSGD iter. 160/499: loss=5.311549677255758, w0=72.68217821782189, w1=15.798037476363207\n",
      "SubSGD iter. 161/499: loss=5.311527362448647, w0=72.68217821782189, w1=15.801989737105961\n",
      "SubSGD iter. 162/499: loss=5.311505047641534, w0=72.68217821782189, w1=15.805941997848715\n",
      "SubSGD iter. 163/499: loss=5.311482732834422, w0=72.68217821782189, w1=15.809894258591468\n",
      "SubSGD iter. 164/499: loss=5.311460418027309, w0=72.68217821782189, w1=15.813846519334222\n",
      "SubSGD iter. 165/499: loss=5.311438103220198, w0=72.68217821782189, w1=15.817798780076975\n",
      "SubSGD iter. 166/499: loss=5.311415788413085, w0=72.68217821782189, w1=15.821751040819729\n",
      "SubSGD iter. 167/499: loss=5.311393473605972, w0=72.68217821782189, w1=15.825703301562482\n",
      "SubSGD iter. 168/499: loss=5.31137115879886, w0=72.68217821782189, w1=15.829655562305236\n",
      "SubSGD iter. 169/499: loss=5.311348843991747, w0=72.68217821782189, w1=15.83360782304799\n",
      "SubSGD iter. 170/499: loss=5.311326529184636, w0=72.68217821782189, w1=15.837560083790743\n",
      "SubSGD iter. 171/499: loss=5.311304214377523, w0=72.68217821782189, w1=15.841512344533497\n",
      "SubSGD iter. 172/499: loss=5.311281899570409, w0=72.68217821782189, w1=15.84546460527625\n",
      "SubSGD iter. 173/499: loss=5.311259584763298, w0=72.68217821782189, w1=15.849416866019004\n",
      "SubSGD iter. 174/499: loss=5.311237269956185, w0=72.68217821782189, w1=15.853369126761757\n",
      "SubSGD iter. 175/499: loss=5.311214955149072, w0=72.68217821782189, w1=15.857321387504511\n",
      "SubSGD iter. 176/499: loss=5.31119264034196, w0=72.68217821782189, w1=15.861273648247264\n",
      "SubSGD iter. 177/499: loss=5.311170325534848, w0=72.68217821782189, w1=15.865225908990018\n",
      "SubSGD iter. 178/499: loss=5.311148010727736, w0=72.68217821782189, w1=15.869178169732772\n",
      "SubSGD iter. 179/499: loss=5.311125695920624, w0=72.68217821782189, w1=15.873130430475525\n",
      "SubSGD iter. 180/499: loss=5.31110338111351, w0=72.68217821782189, w1=15.877082691218279\n",
      "SubSGD iter. 181/499: loss=5.311081066306399, w0=72.68217821782189, w1=15.881034951961032\n",
      "SubSGD iter. 182/499: loss=5.311058751499285, w0=72.68217821782189, w1=15.884987212703786\n",
      "SubSGD iter. 183/499: loss=5.3110364366921745, w0=72.68217821782189, w1=15.88893947344654\n",
      "SubSGD iter. 184/499: loss=5.311014121885061, w0=72.68217821782189, w1=15.892891734189293\n",
      "SubSGD iter. 185/499: loss=5.310991807077948, w0=72.68217821782189, w1=15.896843994932047\n",
      "SubSGD iter. 186/499: loss=5.310969492270837, w0=72.68217821782189, w1=15.9007962556748\n",
      "SubSGD iter. 187/499: loss=5.310947177463723, w0=72.68217821782189, w1=15.904748516417554\n",
      "SubSGD iter. 188/499: loss=5.310924862656612, w0=72.68217821782189, w1=15.908700777160307\n",
      "SubSGD iter. 189/499: loss=5.310902547849499, w0=72.68217821782189, w1=15.91265303790306\n",
      "SubSGD iter. 190/499: loss=5.310913706061381, w0=72.67524752475258, w1=15.910526938117323\n",
      "SubSGD iter. 191/499: loss=5.310892237186269, w0=72.67524752475258, w1=15.914479198860077\n",
      "SubSGD iter. 192/499: loss=5.3108699223791564, w0=72.67524752475258, w1=15.91843145960283\n",
      "SubSGD iter. 193/499: loss=5.310862636053835, w0=72.66831683168327, w1=15.916305359817093\n",
      "SubSGD iter. 194/499: loss=5.310859611715927, w0=72.66831683168327, w1=15.920257620559847\n",
      "SubSGD iter. 195/499: loss=5.310837296908816, w0=72.66831683168327, w1=15.9242098813026\n",
      "SubSGD iter. 196/499: loss=5.310814982101703, w0=72.66831683168327, w1=15.928162142045354\n",
      "SubSGD iter. 197/499: loss=5.310823570190169, w0=72.66138613861396, w1=15.926036042259616\n",
      "SubSGD iter. 198/499: loss=5.310804671438473, w0=72.66138613861396, w1=15.92998830300237\n",
      "SubSGD iter. 199/499: loss=5.3107823566313614, w0=72.66138613861396, w1=15.933940563745123\n",
      "SubSGD iter. 200/499: loss=5.310772500182623, w0=72.65445544554466, w1=15.931814463959386\n",
      "SubSGD iter. 201/499: loss=5.3107720459681325, w0=72.65445544554466, w1=15.93576672470214\n",
      "SubSGD iter. 202/499: loss=5.310749731161021, w0=72.65445544554466, w1=15.939718985444893\n",
      "SubSGD iter. 203/499: loss=5.310727416353908, w0=72.65445544554466, w1=15.943671246187646\n",
      "SubSGD iter. 204/499: loss=5.310733434318958, w0=72.64752475247535, w1=15.941545146401909\n",
      "SubSGD iter. 205/499: loss=5.310717105690679, w0=72.64752475247535, w1=15.945497407144662\n",
      "SubSGD iter. 206/499: loss=5.3106947908835656, w0=72.64752475247535, w1=15.949449667887416\n",
      "SubSGD iter. 207/499: loss=5.310682364311412, w0=72.64059405940604, w1=15.947323568101679\n",
      "SubSGD iter. 208/499: loss=5.310684480220337, w0=72.64059405940604, w1=15.951275828844432\n",
      "SubSGD iter. 209/499: loss=5.310662165413225, w0=72.64059405940604, w1=15.955228089587186\n",
      "SubSGD iter. 210/499: loss=5.310639850606112, w0=72.64059405940604, w1=15.95918035032994\n",
      "SubSGD iter. 211/499: loss=5.310643298447746, w0=72.63366336633673, w1=15.957054250544202\n",
      "SubSGD iter. 212/499: loss=5.310629539942882, w0=72.63366336633673, w1=15.961006511286955\n",
      "SubSGD iter. 213/499: loss=5.3106072251357705, w0=72.63366336633673, w1=15.964958772029709\n",
      "SubSGD iter. 214/499: loss=5.3105922284402, w0=72.62673267326743, w1=15.962832672243971\n",
      "SubSGD iter. 215/499: loss=5.310633183099026, w0=72.63366336633673, w1=15.967301372051375\n",
      "SubSGD iter. 216/499: loss=5.3105993435850625, w0=72.62673267326743, w1=15.965175272265638\n",
      "SubSGD iter. 217/499: loss=5.31061822827579, w0=72.63366336633673, w1=15.969643972073042\n",
      "SubSGD iter. 218/499: loss=5.310606458729927, w0=72.62673267326743, w1=15.967517872287305\n",
      "SubSGD iter. 219/499: loss=5.3106032734525535, w0=72.63366336633673, w1=15.971986572094709\n",
      "SubSGD iter. 220/499: loss=5.310613573874789, w0=72.62673267326743, w1=15.969860472308971\n",
      "SubSGD iter. 221/499: loss=5.310588318629318, w0=72.63366336633673, w1=15.974329172116375\n",
      "SubSGD iter. 222/499: loss=5.310620689019651, w0=72.62673267326743, w1=15.972203072330638\n",
      "SubSGD iter. 223/499: loss=5.3105749661498844, w0=72.62673267326743, w1=15.970593411609551\n",
      "SubSGD iter. 224/499: loss=5.31058363964973, w0=72.63366336633673, w1=15.975062111416955\n",
      "SubSGD iter. 225/499: loss=5.310622915165495, w0=72.62673267326743, w1=15.972936011631218\n",
      "SubSGD iter. 226/499: loss=5.310576651555033, w0=72.62673267326743, w1=15.97132635091013\n",
      "SubSGD iter. 227/499: loss=5.310578960670142, w0=72.63366336633673, w1=15.975795050717535\n",
      "SubSGD iter. 228/499: loss=5.310625141311338, w0=72.62673267326743, w1=15.973668950931797\n",
      "SubSGD iter. 229/499: loss=5.31057833696018, w0=72.62673267326743, w1=15.97205929021071\n",
      "SubSGD iter. 230/499: loss=5.310574635520699, w0=72.62673267326743, w1=15.970449629489623\n",
      "SubSGD iter. 231/499: loss=5.310584557534202, w0=72.63366336633673, w1=15.974918329297028\n",
      "SubSGD iter. 232/499: loss=5.31062247845816, w0=72.62673267326743, w1=15.97279222951129\n",
      "SubSGD iter. 233/499: loss=5.310576320925847, w0=72.62673267326743, w1=15.971182568790203\n",
      "SubSGD iter. 234/499: loss=5.3105798785546146, w0=72.63366336633673, w1=15.975651268597607\n",
      "SubSGD iter. 235/499: loss=5.310624704604003, w0=72.62673267326743, w1=15.97352516881187\n",
      "SubSGD iter. 236/499: loss=5.310578006330994, w0=72.62673267326743, w1=15.971915508090783\n",
      "SubSGD iter. 237/499: loss=5.310575199575027, w0=72.63366336633673, w1=15.976384207898187\n",
      "SubSGD iter. 238/499: loss=5.310626930749847, w0=72.62673267326743, w1=15.97425810811245\n",
      "SubSGD iter. 239/499: loss=5.31057969173614, w0=72.62673267326743, w1=15.972648447391363\n",
      "SubSGD iter. 240/499: loss=5.310575990296659, w0=72.62673267326743, w1=15.971038786670276\n",
      "SubSGD iter. 241/499: loss=5.310580796439088, w0=72.63366336633673, w1=15.97550748647768\n",
      "SubSGD iter. 242/499: loss=5.310624267896667, w0=72.62673267326743, w1=15.973381386691942\n",
      "SubSGD iter. 243/499: loss=5.310577675701808, w0=72.62673267326743, w1=15.971771725970855\n",
      "SubSGD iter. 244/499: loss=5.310576117459501, w0=72.63366336633673, w1=15.97624042577826\n",
      "SubSGD iter. 245/499: loss=5.31062649404251, w0=72.62673267326743, w1=15.974114325992522\n",
      "SubSGD iter. 246/499: loss=5.310579361106954, w0=72.62673267326743, w1=15.972504665271435\n",
      "SubSGD iter. 247/499: loss=5.310575659667472, w0=72.62673267326743, w1=15.970895004550348\n",
      "SubSGD iter. 248/499: loss=5.310581714323563, w0=72.63366336633673, w1=15.975363704357752\n",
      "SubSGD iter. 249/499: loss=5.310623831189332, w0=72.62673267326743, w1=15.973237604572015\n",
      "SubSGD iter. 250/499: loss=5.310577345072619, w0=72.62673267326743, w1=15.971627943850928\n",
      "SubSGD iter. 251/499: loss=5.310577035343975, w0=72.63366336633673, w1=15.976096643658332\n",
      "SubSGD iter. 252/499: loss=5.310626057335176, w0=72.62673267326743, w1=15.973970543872595\n",
      "SubSGD iter. 253/499: loss=5.310579030477766, w0=72.62673267326743, w1=15.972360883151508\n",
      "SubSGD iter. 254/499: loss=5.3105753290382856, w0=72.62673267326743, w1=15.97075122243042\n",
      "SubSGD iter. 255/499: loss=5.310582632208036, w0=72.63366336633673, w1=15.975219922237825\n",
      "SubSGD iter. 256/499: loss=5.310623394481999, w0=72.62673267326743, w1=15.973093822452087\n",
      "SubSGD iter. 257/499: loss=5.3105770144434326, w0=72.62673267326743, w1=15.971484161731\n",
      "SubSGD iter. 258/499: loss=5.3105779532284485, w0=72.63366336633673, w1=15.975952861538405\n",
      "SubSGD iter. 259/499: loss=5.3106256206278415, w0=72.62673267326743, w1=15.973826761752667\n",
      "SubSGD iter. 260/499: loss=5.31057869984858, w0=72.62673267326743, w1=15.97221710103158\n",
      "SubSGD iter. 261/499: loss=5.310574998409098, w0=72.62673267326743, w1=15.970607440310493\n",
      "SubSGD iter. 262/499: loss=5.3105835500925105, w0=72.63366336633673, w1=15.975076140117897\n",
      "SubSGD iter. 263/499: loss=5.3106229577746635, w0=72.62673267326743, w1=15.97295004033216\n",
      "SubSGD iter. 264/499: loss=5.310576683814244, w0=72.62673267326743, w1=15.971340379611073\n",
      "SubSGD iter. 265/499: loss=5.310578871112923, w0=72.63366336633673, w1=15.975809079418477\n",
      "SubSGD iter. 266/499: loss=5.310625183920505, w0=72.62673267326743, w1=15.97368297963274\n",
      "SubSGD iter. 267/499: loss=5.310578369219393, w0=72.62673267326743, w1=15.972073318911653\n",
      "SubSGD iter. 268/499: loss=5.310574667779911, w0=72.62673267326743, w1=15.970463658190566\n",
      "SubSGD iter. 269/499: loss=5.310584467976984, w0=72.63366336633673, w1=15.97493235799797\n",
      "SubSGD iter. 270/499: loss=5.3106225210673275, w0=72.62673267326743, w1=15.972806258212232\n",
      "SubSGD iter. 271/499: loss=5.310576353185058, w0=72.62673267326743, w1=15.971196597491145\n",
      "SubSGD iter. 272/499: loss=5.310579788997396, w0=72.63366336633673, w1=15.97566529729855\n",
      "SubSGD iter. 273/499: loss=5.310624747213171, w0=72.62673267326743, w1=15.973539197512812\n",
      "SubSGD iter. 274/499: loss=5.310578038590206, w0=72.62673267326743, w1=15.971929536791725\n",
      "SubSGD iter. 275/499: loss=5.310575110017808, w0=72.63366336633673, w1=15.97639823659913\n",
      "SubSGD iter. 276/499: loss=5.310626973359014, w0=72.62673267326743, w1=15.974272136813392\n",
      "SubSGD iter. 277/499: loss=5.310579723995353, w0=72.62673267326743, w1=15.972662476092305\n",
      "SubSGD iter. 278/499: loss=5.310576022555871, w0=72.62673267326743, w1=15.971052815371218\n",
      "SubSGD iter. 279/499: loss=5.31058070688187, w0=72.63366336633673, w1=15.975521515178622\n",
      "SubSGD iter. 280/499: loss=5.310624310505837, w0=72.62673267326743, w1=15.973395415392885\n",
      "SubSGD iter. 281/499: loss=5.310577707961019, w0=72.62673267326743, w1=15.971785754671798\n",
      "SubSGD iter. 282/499: loss=5.310576027902282, w0=72.63366336633673, w1=15.976254454479202\n",
      "SubSGD iter. 283/499: loss=5.31062653665168, w0=72.62673267326743, w1=15.974128354693464\n",
      "SubSGD iter. 284/499: loss=5.310579393366166, w0=72.62673267326743, w1=15.972518693972377\n",
      "SubSGD iter. 285/499: loss=5.310575691926685, w0=72.62673267326743, w1=15.97090903325129\n",
      "SubSGD iter. 286/499: loss=5.3105816247663435, w0=72.63366336633673, w1=15.975377733058695\n",
      "SubSGD iter. 287/499: loss=5.310623873798502, w0=72.62673267326743, w1=15.973251633272957\n",
      "SubSGD iter. 288/499: loss=5.310577377331832, w0=72.62673267326743, w1=15.97164197255187\n",
      "SubSGD iter. 289/499: loss=5.310576945786757, w0=72.63366336633673, w1=15.976110672359274\n",
      "SubSGD iter. 290/499: loss=5.3106260999443435, w0=72.62673267326743, w1=15.973984572573537\n",
      "SubSGD iter. 291/499: loss=5.310579062736979, w0=72.62673267326743, w1=15.97237491185245\n",
      "SubSGD iter. 292/499: loss=5.310575361297499, w0=72.62673267326743, w1=15.970765251131363\n",
      "SubSGD iter. 293/499: loss=5.310582542650818, w0=72.63366336633673, w1=15.975233950938767\n",
      "SubSGD iter. 294/499: loss=5.310623437091167, w0=72.62673267326743, w1=15.97310785115303\n",
      "SubSGD iter. 295/499: loss=5.310577046702645, w0=72.62673267326743, w1=15.971498190431943\n",
      "SubSGD iter. 296/499: loss=5.31057786367123, w0=72.63366336633673, w1=15.975966890239347\n",
      "SubSGD iter. 297/499: loss=5.310625663237008, w0=72.62673267326743, w1=15.97384079045361\n",
      "SubSGD iter. 298/499: loss=5.310578732107793, w0=72.62673267326743, w1=15.972231129732522\n",
      "SubSGD iter. 299/499: loss=5.310575030668312, w0=72.62673267326743, w1=15.970621469011435\n",
      "SubSGD iter. 300/499: loss=5.31058346053529, w0=72.63366336633673, w1=15.97509016881884\n",
      "SubSGD iter. 301/499: loss=5.310623000383831, w0=72.62673267326743, w1=15.972964069033102\n",
      "SubSGD iter. 302/499: loss=5.3105767160734585, w0=72.62673267326743, w1=15.971354408312015\n",
      "SubSGD iter. 303/499: loss=5.310578781555702, w0=72.63366336633673, w1=15.97582310811942\n",
      "SubSGD iter. 304/499: loss=5.310625226529674, w0=72.62673267326743, w1=15.973697008333682\n",
      "SubSGD iter. 305/499: loss=5.3105784014786055, w0=72.62673267326743, w1=15.972087347612595\n",
      "SubSGD iter. 306/499: loss=5.310574700039124, w0=72.62673267326743, w1=15.970477686891508\n",
      "SubSGD iter. 307/499: loss=5.310584378419764, w0=72.63366336633673, w1=15.974946386698912\n",
      "SubSGD iter. 308/499: loss=5.310622563676499, w0=72.62673267326743, w1=15.972820286913175\n",
      "SubSGD iter. 309/499: loss=5.3105763854442705, w0=72.62673267326743, w1=15.971210626192088\n",
      "SubSGD iter. 310/499: loss=5.310579699440178, w0=72.63366336633673, w1=15.975679325999492\n",
      "SubSGD iter. 311/499: loss=5.31062478982234, w0=72.62673267326743, w1=15.973553226213754\n",
      "SubSGD iter. 312/499: loss=5.3105780708494175, w0=72.62673267326743, w1=15.971943565492667\n",
      "SubSGD iter. 313/499: loss=5.310575020460591, w0=72.63366336633673, w1=15.976412265300072\n",
      "SubSGD iter. 314/499: loss=5.3106270159681825, w0=72.62673267326743, w1=15.974286165514334\n",
      "SubSGD iter. 315/499: loss=5.310579756254566, w0=72.62673267326743, w1=15.972676504793247\n",
      "SubSGD iter. 316/499: loss=5.310576054815085, w0=72.62673267326743, w1=15.97106684407216\n",
      "SubSGD iter. 317/499: loss=5.310580617324651, w0=72.63366336633673, w1=15.975535543879564\n",
      "SubSGD iter. 318/499: loss=5.3106243531150055, w0=72.62673267326743, w1=15.973409444093827\n",
      "SubSGD iter. 319/499: loss=5.310577740220231, w0=72.62673267326743, w1=15.97179978337274\n",
      "SubSGD iter. 320/499: loss=5.310575938345063, w0=72.63366336633673, w1=15.976268483180144\n",
      "SubSGD iter. 321/499: loss=5.310626579260847, w0=72.62673267326743, w1=15.974142383394407\n",
      "SubSGD iter. 322/499: loss=5.31057942562538, w0=72.62673267326743, w1=15.97253272267332\n",
      "SubSGD iter. 323/499: loss=5.310575724185897, w0=72.62673267326743, w1=15.970923061952233\n",
      "SubSGD iter. 324/499: loss=5.310581535209124, w0=72.63366336633673, w1=15.975391761759637\n",
      "SubSGD iter. 325/499: loss=5.310623916407671, w0=72.62673267326743, w1=15.9732656619739\n",
      "SubSGD iter. 326/499: loss=5.310577409591045, w0=72.62673267326743, w1=15.971656001252812\n",
      "SubSGD iter. 327/499: loss=5.310576856229536, w0=72.63366336633673, w1=15.976124701060217\n",
      "SubSGD iter. 328/499: loss=5.310626142553512, w0=72.62673267326743, w1=15.973998601274479\n",
      "SubSGD iter. 329/499: loss=5.310579094996192, w0=72.62673267326743, w1=15.972388940553392\n",
      "SubSGD iter. 330/499: loss=5.31057539355671, w0=72.62673267326743, w1=15.970779279832305\n",
      "SubSGD iter. 331/499: loss=5.310582453093599, w0=72.63366336633673, w1=15.97524797963971\n",
      "SubSGD iter. 332/499: loss=5.310623479700335, w0=72.62673267326743, w1=15.973121879853972\n",
      "SubSGD iter. 333/499: loss=5.310577078961858, w0=72.62673267326743, w1=15.971512219132885\n",
      "SubSGD iter. 334/499: loss=5.3105777741140106, w0=72.63366336633673, w1=15.975980918940289\n",
      "SubSGD iter. 335/499: loss=5.310625705846178, w0=72.62673267326743, w1=15.973854819154552\n",
      "SubSGD iter. 336/499: loss=5.310578764367005, w0=72.62673267326743, w1=15.972245158433465\n",
      "SubSGD iter. 337/499: loss=5.310575062927524, w0=72.62673267326743, w1=15.970635497712378\n",
      "SubSGD iter. 338/499: loss=5.3105833709780725, w0=72.63366336633673, w1=15.975104197519782\n",
      "SubSGD iter. 339/499: loss=5.310623042993, w0=72.62673267326743, w1=15.972978097734044\n",
      "SubSGD iter. 340/499: loss=5.31057674833267, w0=72.62673267326743, w1=15.971368437012957\n",
      "SubSGD iter. 341/499: loss=5.310578691998486, w0=72.63366336633673, w1=15.975837136820362\n",
      "SubSGD iter. 342/499: loss=5.310625269138844, w0=72.62673267326743, w1=15.973711037034624\n",
      "SubSGD iter. 343/499: loss=5.310578433737819, w0=72.62673267326743, w1=15.972101376313537\n",
      "SubSGD iter. 344/499: loss=5.3105747322983365, w0=72.62673267326743, w1=15.97049171559245\n",
      "SubSGD iter. 345/499: loss=5.310584288862546, w0=72.63366336633673, w1=15.974960415399854\n",
      "SubSGD iter. 346/499: loss=5.3106226062856665, w0=72.62673267326743, w1=15.972834315614117\n",
      "SubSGD iter. 347/499: loss=5.310576417703483, w0=72.62673267326743, w1=15.97122465489303\n",
      "SubSGD iter. 348/499: loss=5.310579609882959, w0=72.63366336633673, w1=15.975693354700434\n",
      "SubSGD iter. 349/499: loss=5.310624832431509, w0=72.62673267326743, w1=15.973567254914697\n",
      "SubSGD iter. 350/499: loss=5.3105781031086305, w0=72.62673267326743, w1=15.97195759419361\n",
      "SubSGD iter. 351/499: loss=5.310574930903372, w0=72.63366336633673, w1=15.976426294001014\n",
      "SubSGD iter. 352/499: loss=5.310627058577351, w0=72.62673267326743, w1=15.974300194215276\n",
      "SubSGD iter. 353/499: loss=5.3105797885137775, w0=72.62673267326743, w1=15.97269053349419\n",
      "SubSGD iter. 354/499: loss=5.310576087074297, w0=72.62673267326743, w1=15.971080872773102\n",
      "SubSGD iter. 355/499: loss=5.310580527767432, w0=72.63366336633673, w1=15.975549572580507\n",
      "SubSGD iter. 356/499: loss=5.310624395724173, w0=72.62673267326743, w1=15.973423472794769\n",
      "SubSGD iter. 357/499: loss=5.310577772479444, w0=72.62673267326743, w1=15.971813812073682\n",
      "SubSGD iter. 358/499: loss=5.3105758487878445, w0=72.63366336633673, w1=15.976282511881086\n",
      "SubSGD iter. 359/499: loss=5.310626621870017, w0=72.62673267326743, w1=15.974156412095349\n",
      "SubSGD iter. 360/499: loss=5.31057945788459, w0=72.62673267326743, w1=15.972546751374262\n",
      "SubSGD iter. 361/499: loss=5.31057575644511, w0=72.62673267326743, w1=15.970937090653175\n",
      "SubSGD iter. 362/499: loss=5.310581445651906, w0=72.63366336633673, w1=15.975405790460579\n",
      "SubSGD iter. 363/499: loss=5.310623959016839, w0=72.62673267326743, w1=15.973279690674842\n",
      "SubSGD iter. 364/499: loss=5.310577441850257, w0=72.62673267326743, w1=15.971670029953755\n",
      "SubSGD iter. 365/499: loss=5.310576766672319, w0=72.63366336633673, w1=15.976138729761159\n",
      "SubSGD iter. 366/499: loss=5.310626185162682, w0=72.62673267326743, w1=15.974012629975421\n",
      "SubSGD iter. 367/499: loss=5.310579127255404, w0=72.62673267326743, w1=15.972402969254334\n",
      "SubSGD iter. 368/499: loss=5.310575425815924, w0=72.62673267326743, w1=15.970793308533247\n",
      "SubSGD iter. 369/499: loss=5.310582363536381, w0=72.63366336633673, w1=15.975262008340652\n",
      "SubSGD iter. 370/499: loss=5.310623522309504, w0=72.62673267326743, w1=15.973135908554914\n",
      "SubSGD iter. 371/499: loss=5.31057711122107, w0=72.62673267326743, w1=15.971526247833827\n",
      "SubSGD iter. 372/499: loss=5.310577684556793, w0=72.63366336633673, w1=15.975994947641231\n",
      "SubSGD iter. 373/499: loss=5.310625748455347, w0=72.62673267326743, w1=15.973868847855494\n",
      "SubSGD iter. 374/499: loss=5.310578796626218, w0=72.62673267326743, w1=15.972259187134407\n",
      "SubSGD iter. 375/499: loss=5.310575095186736, w0=72.62673267326743, w1=15.97064952641332\n",
      "SubSGD iter. 376/499: loss=5.310583281420854, w0=72.63366336633673, w1=15.975118226220724\n",
      "SubSGD iter. 377/499: loss=5.3106230856021694, w0=72.62673267326743, w1=15.972992126434987\n",
      "SubSGD iter. 378/499: loss=5.310576780591885, w0=72.62673267326743, w1=15.9713824657139\n",
      "SubSGD iter. 379/499: loss=5.310578602441266, w0=72.63366336633673, w1=15.975851165521304\n",
      "SubSGD iter. 380/499: loss=5.310625311748012, w0=72.62673267326743, w1=15.973725065735566\n",
      "SubSGD iter. 381/499: loss=5.310578465997031, w0=72.62673267326743, w1=15.97211540501448\n",
      "SubSGD iter. 382/499: loss=5.31057476455755, w0=72.62673267326743, w1=15.970505744293392\n",
      "SubSGD iter. 383/499: loss=5.310584199305326, w0=72.63366336633673, w1=15.974974444100797\n",
      "SubSGD iter. 384/499: loss=5.310622648894835, w0=72.62673267326743, w1=15.972848344315059\n",
      "SubSGD iter. 385/499: loss=5.310576449962697, w0=72.62673267326743, w1=15.971238683593972\n",
      "SubSGD iter. 386/499: loss=5.31057952032574, w0=72.63366336633673, w1=15.975707383401376\n",
      "SubSGD iter. 387/499: loss=5.310624875040677, w0=72.62673267326743, w1=15.973581283615639\n",
      "SubSGD iter. 388/499: loss=5.3105781353678445, w0=72.62673267326743, w1=15.971971622894552\n",
      "SubSGD iter. 389/499: loss=5.310574841346153, w0=72.63366336633673, w1=15.976440322701956\n",
      "SubSGD iter. 390/499: loss=5.31062710118652, w0=72.62673267326743, w1=15.974314222916218\n",
      "SubSGD iter. 391/499: loss=5.3105798207729915, w0=72.62673267326743, w1=15.972704562195132\n",
      "SubSGD iter. 392/499: loss=5.3105761193335095, w0=72.62673267326743, w1=15.971094901474045\n",
      "SubSGD iter. 393/499: loss=5.310580438210215, w0=72.63366336633673, w1=15.975563601281449\n",
      "SubSGD iter. 394/499: loss=5.310624438333343, w0=72.62673267326743, w1=15.973437501495711\n",
      "SubSGD iter. 395/499: loss=5.3105778047386565, w0=72.62673267326743, w1=15.971827840774624\n",
      "SubSGD iter. 396/499: loss=5.310575759230627, w0=72.63366336633673, w1=15.976296540582029\n",
      "SubSGD iter. 397/499: loss=5.310626664479185, w0=72.62673267326743, w1=15.974170440796291\n",
      "SubSGD iter. 398/499: loss=5.3105794901438035, w0=72.62673267326743, w1=15.972560780075204\n",
      "SubSGD iter. 399/499: loss=5.310575788704323, w0=72.62673267326743, w1=15.970951119354117\n",
      "SubSGD iter. 400/499: loss=5.310581356094687, w0=72.63366336633673, w1=15.975419819161521\n",
      "SubSGD iter. 401/499: loss=5.310624001626008, w0=72.62673267326743, w1=15.973293719375784\n",
      "SubSGD iter. 402/499: loss=5.31057747410947, w0=72.62673267326743, w1=15.971684058654697\n",
      "SubSGD iter. 403/499: loss=5.310576677115099, w0=72.63366336633673, w1=15.976152758462101\n",
      "SubSGD iter. 404/499: loss=5.31062622777185, w0=72.62673267326743, w1=15.974026658676364\n",
      "SubSGD iter. 405/499: loss=5.310579159514617, w0=72.62673267326743, w1=15.972416997955277\n",
      "SubSGD iter. 406/499: loss=5.310575458075135, w0=72.62673267326743, w1=15.97080733723419\n",
      "SubSGD iter. 407/499: loss=5.310582273979162, w0=72.63366336633673, w1=15.975276037041594\n",
      "SubSGD iter. 408/499: loss=5.310623564918673, w0=72.62673267326743, w1=15.973149937255856\n",
      "SubSGD iter. 409/499: loss=5.310577143480284, w0=72.62673267326743, w1=15.97154027653477\n",
      "SubSGD iter. 410/499: loss=5.3105775949995735, w0=72.63366336633673, w1=15.976008976342174\n",
      "SubSGD iter. 411/499: loss=5.310625791064514, w0=72.62673267326743, w1=15.973882876556436\n",
      "SubSGD iter. 412/499: loss=5.31057882888543, w0=72.62673267326743, w1=15.972273215835349\n",
      "SubSGD iter. 413/499: loss=5.310575127445949, w0=72.62673267326743, w1=15.970663555114262\n",
      "SubSGD iter. 414/499: loss=5.310583191863635, w0=72.63366336633673, w1=15.975132254921666\n",
      "SubSGD iter. 415/499: loss=5.310623128211338, w0=72.62673267326743, w1=15.973006155135929\n",
      "SubSGD iter. 416/499: loss=5.310576812851096, w0=72.62673267326743, w1=15.971396494414842\n",
      "SubSGD iter. 417/499: loss=5.310578512884048, w0=72.63366336633673, w1=15.975865194222246\n",
      "SubSGD iter. 418/499: loss=5.31062535435718, w0=72.62673267326743, w1=15.973739094436509\n",
      "SubSGD iter. 419/499: loss=5.310578498256244, w0=72.62673267326743, w1=15.972129433715422\n",
      "SubSGD iter. 420/499: loss=5.310574796816762, w0=72.62673267326743, w1=15.970519772994335\n",
      "SubSGD iter. 421/499: loss=5.31058410974811, w0=72.63366336633673, w1=15.974988472801739\n",
      "SubSGD iter. 422/499: loss=5.310622691504003, w0=72.62673267326743, w1=15.972862373016001\n",
      "SubSGD iter. 423/499: loss=5.310576482221909, w0=72.62673267326743, w1=15.971252712294914\n",
      "SubSGD iter. 424/499: loss=5.310579430768521, w0=72.63366336633673, w1=15.975721412102319\n",
      "SubSGD iter. 425/499: loss=5.310624917649847, w0=72.62673267326743, w1=15.973595312316581\n",
      "SubSGD iter. 426/499: loss=5.310578167627056, w0=72.62673267326743, w1=15.971985651595494\n",
      "SubSGD iter. 427/499: loss=5.310574751788934, w0=72.63366336633673, w1=15.976454351402898\n",
      "SubSGD iter. 428/499: loss=5.310627143795689, w0=72.62673267326743, w1=15.97432825161716\n",
      "SubSGD iter. 429/499: loss=5.310579853032204, w0=72.62673267326743, w1=15.972718590896074\n",
      "SubSGD iter. 430/499: loss=5.310576151592722, w0=72.62673267326743, w1=15.971108930174987\n",
      "SubSGD iter. 431/499: loss=5.310580348652994, w0=72.63366336633673, w1=15.975577629982391\n",
      "SubSGD iter. 432/499: loss=5.310624480942511, w0=72.62673267326743, w1=15.973451530196654\n",
      "SubSGD iter. 433/499: loss=5.31057783699787, w0=72.62673267326743, w1=15.971841869475567\n",
      "SubSGD iter. 434/499: loss=5.3105756696734066, w0=72.63366336633673, w1=15.97631056928297\n",
      "SubSGD iter. 435/499: loss=5.310626707088354, w0=72.62673267326743, w1=15.974184469497233\n",
      "SubSGD iter. 436/499: loss=5.310579522403018, w0=72.62673267326743, w1=15.972574808776146\n",
      "SubSGD iter. 437/499: loss=5.310575820963536, w0=72.62673267326743, w1=15.97096514805506\n",
      "SubSGD iter. 438/499: loss=5.3105812665374685, w0=72.63366336633673, w1=15.975433847862464\n",
      "SubSGD iter. 439/499: loss=5.310624044235177, w0=72.62673267326743, w1=15.973307748076726\n",
      "SubSGD iter. 440/499: loss=5.310577506368682, w0=72.62673267326743, w1=15.97169808735564\n",
      "SubSGD iter. 441/499: loss=5.31057658755788, w0=72.63366336633673, w1=15.976166787163043\n",
      "SubSGD iter. 442/499: loss=5.310626270381019, w0=72.62673267326743, w1=15.974040687377306\n",
      "SubSGD iter. 443/499: loss=5.31057919177383, w0=72.62673267326743, w1=15.972431026656219\n",
      "SubSGD iter. 444/499: loss=5.310575490334348, w0=72.62673267326743, w1=15.970821365935132\n",
      "SubSGD iter. 445/499: loss=5.310582184421943, w0=72.63366336633673, w1=15.975290065742536\n",
      "SubSGD iter. 446/499: loss=5.310623607527842, w0=72.62673267326743, w1=15.973163965956799\n",
      "SubSGD iter. 447/499: loss=5.310577175739496, w0=72.62673267326743, w1=15.971554305235712\n",
      "SubSGD iter. 448/499: loss=5.310577505442355, w0=72.63366336633673, w1=15.976023005043116\n",
      "SubSGD iter. 449/499: loss=5.310625833673684, w0=72.62673267326743, w1=15.973896905257378\n",
      "SubSGD iter. 450/499: loss=5.310578861144643, w0=72.62673267326743, w1=15.972287244536291\n",
      "SubSGD iter. 451/499: loss=5.310575159705162, w0=72.62673267326743, w1=15.970677583815204\n",
      "SubSGD iter. 452/499: loss=5.310583102306416, w0=72.63366336633673, w1=15.975146283622609\n",
      "SubSGD iter. 453/499: loss=5.310623170820506, w0=72.62673267326743, w1=15.973020183836871\n",
      "SubSGD iter. 454/499: loss=5.310576845110308, w0=72.62673267326743, w1=15.971410523115784\n",
      "SubSGD iter. 455/499: loss=5.310578423326827, w0=72.63366336633673, w1=15.975879222923188\n",
      "SubSGD iter. 456/499: loss=5.3106253969663495, w0=72.62673267326743, w1=15.97375312313745\n",
      "SubSGD iter. 457/499: loss=5.310578530515456, w0=72.62673267326743, w1=15.972143462416364\n",
      "SubSGD iter. 458/499: loss=5.310574829075975, w0=72.62673267326743, w1=15.970533801695277\n",
      "SubSGD iter. 459/499: loss=5.3105840201908885, w0=72.63366336633673, w1=15.975002501502681\n",
      "SubSGD iter. 460/499: loss=5.3106227341131715, w0=72.62673267326743, w1=15.972876401716944\n",
      "SubSGD iter. 461/499: loss=5.310576514481122, w0=72.62673267326743, w1=15.971266740995857\n",
      "SubSGD iter. 462/499: loss=5.3105793412113025, w0=72.63366336633673, w1=15.97573544080326\n",
      "SubSGD iter. 463/499: loss=5.310624960259014, w0=72.62673267326743, w1=15.973609341017523\n",
      "SubSGD iter. 464/499: loss=5.310578199886269, w0=72.62673267326743, w1=15.971999680296436\n",
      "SubSGD iter. 465/499: loss=5.310574662231715, w0=72.63366336633673, w1=15.97646838010384\n",
      "SubSGD iter. 466/499: loss=5.310627186404858, w0=72.62673267326743, w1=15.974342280318103\n",
      "SubSGD iter. 467/499: loss=5.310579885291417, w0=72.62673267326743, w1=15.972732619597016\n",
      "SubSGD iter. 468/499: loss=5.310576183851936, w0=72.62673267326743, w1=15.97112295887593\n",
      "SubSGD iter. 469/499: loss=5.310580259095775, w0=72.63366336633673, w1=15.975591658683333\n",
      "SubSGD iter. 470/499: loss=5.310624523551678, w0=72.62673267326743, w1=15.973465558897596\n",
      "SubSGD iter. 471/499: loss=5.310577869257083, w0=72.62673267326743, w1=15.971855898176509\n",
      "SubSGD iter. 472/499: loss=5.310575580116187, w0=72.63366336633673, w1=15.976324597983913\n",
      "SubSGD iter. 473/499: loss=5.310626749697523, w0=72.62673267326743, w1=15.974198498198175\n",
      "SubSGD iter. 474/499: loss=5.3105795546622305, w0=72.62673267326743, w1=15.972588837477089\n",
      "SubSGD iter. 475/499: loss=5.3105758532227485, w0=72.62673267326743, w1=15.970979176756002\n",
      "SubSGD iter. 476/499: loss=5.310581176980249, w0=72.63366336633673, w1=15.975447876563406\n",
      "SubSGD iter. 477/499: loss=5.310624086844345, w0=72.62673267326743, w1=15.973321776777668\n",
      "SubSGD iter. 478/499: loss=5.3105775386278955, w0=72.62673267326743, w1=15.971712116056581\n",
      "SubSGD iter. 479/499: loss=5.3105764980006605, w0=72.63366336633673, w1=15.976180815863986\n",
      "SubSGD iter. 480/499: loss=5.310626312990188, w0=72.62673267326743, w1=15.974054716078248\n",
      "SubSGD iter. 481/499: loss=5.3105792240330425, w0=72.62673267326743, w1=15.972445055357161\n",
      "SubSGD iter. 482/499: loss=5.3105755225935605, w0=72.62673267326743, w1=15.970835394636074\n",
      "SubSGD iter. 483/499: loss=5.310582094864723, w0=72.63366336633673, w1=15.975304094443478\n",
      "SubSGD iter. 484/499: loss=5.31062365013701, w0=72.62673267326743, w1=15.97317799465774\n",
      "SubSGD iter. 485/499: loss=5.3105772079987075, w0=72.62673267326743, w1=15.971568333936654\n",
      "SubSGD iter. 486/499: loss=5.3105774158851355, w0=72.63366336633673, w1=15.976037033744058\n",
      "SubSGD iter. 487/499: loss=5.310625876282853, w0=72.62673267326743, w1=15.97391093395832\n",
      "SubSGD iter. 488/499: loss=5.310578893403856, w0=72.62673267326743, w1=15.972301273237234\n",
      "SubSGD iter. 489/499: loss=5.310575191964374, w0=72.62673267326743, w1=15.970691612516147\n",
      "SubSGD iter. 490/499: loss=5.3105830127491975, w0=72.63366336633673, w1=15.97516031232355\n",
      "SubSGD iter. 491/499: loss=5.310623213429675, w0=72.62673267326743, w1=15.973034212537813\n",
      "SubSGD iter. 492/499: loss=5.310576877369521, w0=72.62673267326743, w1=15.971424551816726\n",
      "SubSGD iter. 493/499: loss=5.310578333769609, w0=72.63366336633673, w1=15.97589325162413\n",
      "SubSGD iter. 494/499: loss=5.310625439575518, w0=72.62673267326743, w1=15.973767151838393\n",
      "SubSGD iter. 495/499: loss=5.310578562774669, w0=72.62673267326743, w1=15.972157491117306\n",
      "SubSGD iter. 496/499: loss=5.310574861335188, w0=72.62673267326743, w1=15.97054783039622\n",
      "SubSGD iter. 497/499: loss=5.310583930633672, w0=72.63366336633673, w1=15.975016530203623\n",
      "SubSGD iter. 498/499: loss=5.310622776722341, w0=72.62673267326743, w1=15.972890430417886\n",
      "SubSGD iter. 499/499: loss=5.310576546740335, w0=72.62673267326743, w1=15.971280769696799\n",
      "SubSGD: execution time=0.041 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9d136d9204490ea8d6c8777fd0c17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses, subsgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "233a531365d7bd5abb8382eb032c18c305e1c6b951add6f6a5c925475bc609cb"
   }
  },
  "widgets": {
   "state": {
    "d2b2c3aea192430e81437f33ba0b0e69": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e4a6a7a70ccd42ddb112989c04f2ed3f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
